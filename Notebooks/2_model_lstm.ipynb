{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing part I did the same steps as with the encoder-decoder model. With the two differences:\n",
    "- selected sequences no longer than 100 amino acids\n",
    "- removed EOS token from the sequences and kept only SOS (which in this case worked only as a padding token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = pd.read_csv(\"../Data/2018-06-06-ss.cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>chain_code</th>\n",
       "      <th>seq</th>\n",
       "      <th>sst8</th>\n",
       "      <th>sst3</th>\n",
       "      <th>len</th>\n",
       "      <th>has_nonstd_aa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1A30</td>\n",
       "      <td>C</td>\n",
       "      <td>EDL</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1B05</td>\n",
       "      <td>B</td>\n",
       "      <td>KCK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1B0H</td>\n",
       "      <td>B</td>\n",
       "      <td>KAK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1B1H</td>\n",
       "      <td>B</td>\n",
       "      <td>KFK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1B2H</td>\n",
       "      <td>B</td>\n",
       "      <td>KAK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393727</th>\n",
       "      <td>4UWE</td>\n",
       "      <td>D</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCBTTCEEEEEEEEEETTEEEEEEEECCCSSCCB...</td>\n",
       "      <td>CCCCCCCCCCCCCCECCCEEEEEEEEEECCEEEEEEEECCCCCCCE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393728</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>A</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393729</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>B</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393730</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>C</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393731</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>D</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393732 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pdb_id chain_code                                                seq  \\\n",
       "0        1A30          C                                                EDL   \n",
       "1        1B05          B                                                KCK   \n",
       "2        1B0H          B                                                KAK   \n",
       "3        1B1H          B                                                KFK   \n",
       "4        1B2H          B                                                KAK   \n",
       "...       ...        ...                                                ...   \n",
       "393727   4UWE          D  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393728   5J8V          A  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393729   5J8V          B  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393730   5J8V          C  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393731   5J8V          D  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "\n",
       "                                                     sst8  \\\n",
       "0                                                     CBC   \n",
       "1                                                     CBC   \n",
       "2                                                     CBC   \n",
       "3                                                     CBC   \n",
       "4                                                     CBC   \n",
       "...                                                   ...   \n",
       "393727  CCCCCCCCCCCCCCBTTCEEEEEEEEEETTEEEEEEEECCCSSCCB...   \n",
       "393728  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393729  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393730  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393731  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "\n",
       "                                                     sst3   len  has_nonstd_aa  \n",
       "0                                                     CEC     3          False  \n",
       "1                                                     CEC     3          False  \n",
       "2                                                     CEC     3          False  \n",
       "3                                                     CEC     3          False  \n",
       "4                                                     CEC     3          False  \n",
       "...                                                   ...   ...            ...  \n",
       "393727  CCCCCCCCCCCCCCECCCEEEEEEEEEECCEEEEEEEECCCCCCCE...  5037           True  \n",
       "393728  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393729  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393730  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393731  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "\n",
       "[393732 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(sequence):\n",
    "    s = set(list(sequence))\n",
    "    if len(s)==1:\n",
    "        letter = s.pop()\n",
    "        if letter == \"*\": \n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = proteins[\n",
    "    (proteins[\"len\"]>=1) &\n",
    "    (proteins[\"len\"]<=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[[\"seq\",\"sst3\",\"sst8\"]]\n",
    "sample = sample.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"remove\"] = sample[\"seq\"].apply(remove_empty) \n",
    "sample = sample[sample[\"remove\"]==0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>sst3</th>\n",
       "      <th>sst8</th>\n",
       "      <th>remove</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EDL</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KCK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KAK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KFK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KMK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61918</th>\n",
       "      <td>MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCSCCCCCHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61920</th>\n",
       "      <td>MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCTTCCCHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61921</th>\n",
       "      <td>RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...</td>\n",
       "      <td>CCCCHHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCHHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61922</th>\n",
       "      <td>RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...</td>\n",
       "      <td>CCCCCHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61923</th>\n",
       "      <td>RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...</td>\n",
       "      <td>CCCCCCHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCCHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42386 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     seq  \\\n",
       "0                                                    EDL   \n",
       "1                                                    KCK   \n",
       "2                                                    KAK   \n",
       "3                                                    KFK   \n",
       "5                                                    KMK   \n",
       "...                                                  ...   \n",
       "61918  MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...   \n",
       "61920  MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...   \n",
       "61921  RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...   \n",
       "61922  RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...   \n",
       "61923  RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...   \n",
       "\n",
       "                                                    sst3  \\\n",
       "0                                                    CEC   \n",
       "1                                                    CEC   \n",
       "2                                                    CEC   \n",
       "3                                                    CEC   \n",
       "5                                                    CEC   \n",
       "...                                                  ...   \n",
       "61918  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...   \n",
       "61920  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...   \n",
       "61921  CCCCHHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...   \n",
       "61922  CCCCCHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...   \n",
       "61923  CCCCCCHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...   \n",
       "\n",
       "                                                    sst8  remove  len  \n",
       "0                                                    CBC       0    3  \n",
       "1                                                    CBC       0    3  \n",
       "2                                                    CBC       0    3  \n",
       "3                                                    CBC       0    3  \n",
       "5                                                    CBC       0    3  \n",
       "...                                                  ...     ...  ...  \n",
       "61918  CCCCCCCCCCCCCCCCCCCCCCCCCCCCSCCCCCHHHHHHHHHHHH...       0  100  \n",
       "61920  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCTTCCCHHHHHHHHHHHH...       0  100  \n",
       "61921  CCCCHHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...       0  100  \n",
       "61922  CCCCCHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...       0  100  \n",
       "61923  CCCCCCHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...       0  100  \n",
       "\n",
       "[42386 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"len\"] = sample[\"seq\"].apply(len)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/input_char2index.json') as f:\n",
    "    input_word2index = json.load(f)\n",
    "\n",
    "with open('../static/input_index2char.json') as f:\n",
    "    input_index2word = json.load(f)\n",
    "\n",
    "\n",
    "with open('../static/output_char2index.json') as f:\n",
    "    output_word2index = json.load(f)\n",
    "\n",
    "with open('../static/output_index2char.json') as f:\n",
    "    output_index2word = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, word2index={}, index2word= {0: \"SOS\"}):\n",
    "        self.word2index = word2index\n",
    "        self.word2count = {}\n",
    "        self.index2word = index2word\n",
    "        self.n_words = len(self.index2word)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in list(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            if word not in self.word2count:\n",
    "                self.word2count[word] = 1\n",
    "            else:\n",
    "                self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "Sequence: 22\n",
      "Structure: 4\n",
      "('MVECPVCGSEIEIGEVELHQIVECPVCGAELEVVSLEPLTLEELPEVEEDWGE', 'CCCCCCCCCCCCCCCCCCCEECCCCCCCCCCEECCCCCCCEECCCCCCCCECC')\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, input_word2index, input_index2word,output_word2index,output_index2word):\n",
    "\n",
    "    input_lang = Lang(input_word2index, input_index2word)\n",
    "    output_lang = Lang(output_word2index,output_index2word) \n",
    "\n",
    "    pairs = list(zip(lang1,lang2))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(f\"Sequence: {input_lang.n_words}\")\n",
    "    print(f\"Structure: {output_lang.n_words}\")\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(sample[\"seq\"], sample[\"sst3\"],input_word2index, input_index2word,output_word2index,output_index2word)\n",
    "\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = sample[\"len\"].max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in list(sentence)]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "n = len(pairs)\n",
    "input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_lang, inp)\n",
    "    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(input_ids)*0.6)\n",
    "test_size = int(len(input_ids)*0.2)\n",
    "\n",
    "X = input_ids\n",
    "y = target_ids\n",
    "\n",
    "X_train = torch.tensor(X[:train_size], dtype=torch.long)\n",
    "y_train = torch.tensor(y[:train_size],dtype=torch.long)\n",
    "\n",
    "X_test = torch.tensor(X[train_size:train_size+test_size],dtype=torch.long)\n",
    "y_test = torch.tensor(y[train_size:train_size+test_size],dtype=torch.long)\n",
    "\n",
    "X_val = torch.tensor(X[train_size+test_size:],dtype=torch.long)\n",
    "y_val = torch.tensor(y[train_size+test_size:],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_freq = (torch.tensor(y, dtype=torch.long).shape[0] * torch.tensor(y, dtype=torch.long).shape[1]) - torch.count_nonzero(torch.tensor(y, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1349, 0.1700, 0.4359, 0.2592])\n"
     ]
    }
   ],
   "source": [
    "vocab = output_lang.word2index\n",
    "word_freq = output_lang.word2count\n",
    "\n",
    "vocab.update({\"SOS\":0})\n",
    "word_freq.update({\"SOS\":int(SOS_freq)})\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "weights = torch.zeros(vocab_size)\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    weights[idx] = 1.0 / (word_freq[word]) \n",
    "    \n",
    "weights = weights / weights.sum()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model I wrote a simple LSTM with embedding. I used bidirectional LSTM as it was more accurate dan unidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inpit_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = F.relu(x)\n",
    "        x , hidden = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim = -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 11,  4,  3, 12,  1, 13, 12,  4, 19,  6,  1,  1, 10, 21, 17, 17, 13,\n",
       "        18, 10, 11,  2, 15,  3,  6, 19,  7,  4, 16,  2, 19, 18,  4,  6,  3,  4,\n",
       "         6,  8,  1,  8, 14, 21, 16, 16,  8,  1,  4,  4,  1,  4,  3,  8, 21, 10,\n",
       "         4,  4,  6,  6,  1,  2, 17,  4, 19, 15,  1, 19,  1,  3, 13,  1,  8, 19,\n",
       "         6, 12, 12,  6,  6, 14, 16, 13, 13,  4,  4,  3,  1,  9,  9,  9,  9,  9,\n",
       "         9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1,\n",
       "        1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "batch_size = 128\n",
    "hidden_size = 256\n",
    "n_epochs = 100\n",
    "\n",
    "patience = 5\n",
    "best_result = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(X_train,y_train)), batch_size=batch_size)\n",
    "test_loader = DataLoader(list(zip(X_test,y_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_lang.n_words, 64, hidden_size, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.6360892123646207, Test loss: 0.5607433319091797\n",
      "Epoch: 1, Train loss: 0.5137996133228745, Test loss: 0.5057401657104492\n",
      "Epoch: 2, Train loss: 0.46189970304869643, Test loss: 0.4459366202354431\n",
      "Epoch: 3, Train loss: 0.4146846993403001, Test loss: 0.40328508615493774\n",
      "Epoch: 4, Train loss: 0.3698088529736105, Test loss: 0.37047678232192993\n",
      "Epoch: 5, Train loss: 0.3357383542590671, Test loss: 0.35007479786872864\n",
      "Epoch: 6, Train loss: 0.30862099428971607, Test loss: 0.3300267159938812\n",
      "Epoch: 7, Train loss: 0.28828926309190617, Test loss: 0.3235602080821991\n",
      "Epoch: 8, Train loss: 0.27139973723226124, Test loss: 0.3023546636104584\n",
      "Epoch: 9, Train loss: 0.25524764282233786, Test loss: 0.2917926609516144\n",
      "Epoch: 10, Train loss: 0.24256782218663378, Test loss: 0.29548197984695435\n",
      "Epoch: 11, Train loss: 0.2320493036749387, Test loss: 0.2873603105545044\n",
      "Epoch: 12, Train loss: 0.22079342076874742, Test loss: 0.28673917055130005\n",
      "Epoch: 13, Train loss: 0.2108140816592207, Test loss: 0.2794758379459381\n",
      "Epoch: 14, Train loss: 0.20212648989576282, Test loss: 0.2801874279975891\n",
      "Epoch: 15, Train loss: 0.19562816032857605, Test loss: 0.2761549651622772\n",
      "Epoch: 16, Train loss: 0.1902024292885655, Test loss: 0.27105870842933655\n",
      "Epoch: 17, Train loss: 0.1861940949703708, Test loss: 0.2728874385356903\n"
     ]
    }
   ],
   "source": [
    "test_loss_array = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(X_batch)\n",
    "\n",
    "        loss = loss_fn(output.permute(1,2,0), y_batch.permute(1,0))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        for X_batch, y_batch in test_loader:\n",
    "\n",
    "            output = model(X_batch)\n",
    "            loss = loss_fn(output.permute(1,2,0), y_batch.permute(1,0))\n",
    "\n",
    "            test_loss+=loss\n",
    "\n",
    "    loss = total_loss / (len(X_train) // batch_size)\n",
    "    loss_test = test_loss / (len(y_test) // batch_size)\n",
    "\n",
    "    test_loss_array.append(loss_test)\n",
    "\n",
    "    if loss_test < best_result:\n",
    "        torch.save(model.state_dict(), \"../Models/lstm_1.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train loss: {loss}, Test loss: {loss_test}\")\n",
    "\n",
    "    if len(test_loss_array)>patience+1:\n",
    "        if not (any(x > (test_loss_array[-1]+0.015) for x in test_loss_array[len(test_loss_array)-patience-1:-1])):\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../Models/lstm_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'SOS', '1': 'C', '2': 'E', '3': 'H'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    outputs_pred = model(X_val)\n",
    "\n",
    "    _, topi = outputs_pred.topk(1)\n",
    "    decoded_ids = topi.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCCCHHHHHCCCCEEEEEECCCCEEEEEEEEECCCCCEEEEEEEEECCCCCEEEEEEECHHHEEEEEC', 'CCCCCCCCCHHHHHHHHHHHHHHHHHHHCCHHHHHHHHHHHHHHHHHCCCCCCCCCCHHHHHHHHHHCCHCCHHHHHHHHHHHHHCCCCC', 'CCHHHHHHHHHHHHHHHHHHHHHHHHHHHCCCC', 'CEEEEEEECCCCCEEEEEEECCCCEEHHHHHHHHHHCCCCCHCHHEEEECCEEECCCEHHHHHCCCCEEEEEEEEC', 'CCCCCEEEEEEECCCCEEEEEECCEEEEEECCCCCCCCCCCCCCCEEEEEEEECCCEEEEEECCCCEEEEECCCC']\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for idx in decoded_ids:\n",
    "    decoded_structure = []\n",
    "    for id in idx:\n",
    "        if id.item() == SOS_token:\n",
    "            break\n",
    "        decoded_structure.append(output_lang.index2word[str(id.item())])\n",
    "    pred.append(\"\".join(decoded_structure))\n",
    "\n",
    "print(pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCCCHHHHCCCCCEEEEEECCCCEEEEEEEEECCCCCEEEEEEEECCCCCCCEEEEEECHHHEEEEEC', 'CCCCCCCCCHHHHHHHHHHHHHHHHHHHCCCHHHCHHHHHHHHHHHHCCCCCCCCCCHHHHHHHHHHCCCCCHHHHHHHHCCCCCCCCCC', 'CCHHHHHHHHHHHHHHHHHHHHHHHHHHHCCCC', 'CEEEEEEECCCCCEEEEEEECCCCEHHHHHHHHHHCCCCCCHHHCEEEECCEECCCCCEHHHCCCCCCCCEEEEEC', 'CCCCCCEEEEEECCCEEEEEECCCCEEECCCCCCCCCCCCCCCCCCCEEEEEECCCEEEEEECCCCEEECCCCCC']\n"
     ]
    }
   ],
   "source": [
    "target=[]\n",
    "for idx in y_val:\n",
    "    decoded_structure = []\n",
    "    for id in idx:\n",
    "        if id.item() == SOS_token:\n",
    "            break\n",
    "        decoded_structure.append(output_lang.index2word[str(id.item())])\n",
    "    target.append(\"\".join(decoded_structure))\n",
    "\n",
    "print(target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are satisfying, as both the accuracy and F1 score are quite high. It is also a great confirmation that the model learned not only the most popular characters but predicts also less frequent characters. The model developed in this notebook will be a great base for the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level accuracy: 86.51860674349227%\n",
      "Character-level f1: 79.26692650361007%\n",
      "Exact match: 5.779665015333805%\n"
     ]
    }
   ],
   "source": [
    "def char_level_metrics(predictions, targets):\n",
    "    accuracy = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    for pred, target in zip(list(predictions), list(targets)):\n",
    "        if len(pred)<len(target):\n",
    "            pred = pred + (\"$\" * (len(target)-len(pred)))\n",
    "        if len(pred)>len(target):\n",
    "            target = target + (\"$\" * (len(pred)-len(target)))\n",
    "\n",
    "        accuracy += accuracy_score(list(pred),list(target))\n",
    "        f1 += f1_score(list(pred),list(target), average=\"macro\")\n",
    "\n",
    "    return accuracy/len(predictions), f1/len(predictions)\n",
    "\n",
    "ac, f1 = char_level_metrics(pred, target)\n",
    "\n",
    "print(f'Character-level accuracy: {ac*100}%')\n",
    "print(f'Character-level f1: {f1*100}%')\n",
    "print(f'Exact match: {accuracy_score(pred,target)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
