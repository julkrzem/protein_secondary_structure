{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I decided to test if the results can improve if the sequences were padded only to the maximal length in a batch and not in the entire dataset. I wanted to train the network in batches sorted by length with the aim of gathering together similar length sequences and padding them as necessary only to the length of longest sequence in a batch. I was hoping that it would minimise the effect of padding on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing was similar to the previous notebook but I have to turn the preprocessing stage into a function and call it during training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = pd.read_csv(\"../Data/2018-06-06-ss.cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(sequence):\n",
    "    s = set(list(sequence))\n",
    "    if len(s)==1:\n",
    "        letter = s.pop()\n",
    "        if letter == \"*\": \n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = proteins[\n",
    "    (proteins[\"len\"]>=1) &\n",
    "    (proteins[\"len\"]<=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = sample[\"len\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[[\"seq\",\"sst3\",\"sst8\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"remove\"] = sample[\"seq\"].apply(remove_empty) \n",
    "sample = sample[sample[\"remove\"]==0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = sample.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>sst3</th>\n",
       "      <th>sst8</th>\n",
       "      <th>remove</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EDL</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KCK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KAK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KFK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KMK</td>\n",
       "      <td>CEC</td>\n",
       "      <td>CBC</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61918</th>\n",
       "      <td>MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCSCCCCCHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61920</th>\n",
       "      <td>MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCTTCCCHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61921</th>\n",
       "      <td>RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...</td>\n",
       "      <td>CCCCHHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCHHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61922</th>\n",
       "      <td>RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...</td>\n",
       "      <td>CCCCCHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61923</th>\n",
       "      <td>RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...</td>\n",
       "      <td>CCCCCCHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>CCCCCCHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42386 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     seq  \\\n",
       "0                                                    EDL   \n",
       "1                                                    KCK   \n",
       "2                                                    KAK   \n",
       "3                                                    KFK   \n",
       "5                                                    KMK   \n",
       "...                                                  ...   \n",
       "61918  MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...   \n",
       "61920  MAVKTGIAIGLNKGKKVTQMTPAPKISYKKGAASNRTKFVRSLVRE...   \n",
       "61921  RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...   \n",
       "61922  RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...   \n",
       "61923  RYNDYKLDFRRQQMQDFFLAHKDEEWFRSKYHPDEVGKRRQEARGA...   \n",
       "\n",
       "                                                    sst3  \\\n",
       "0                                                    CEC   \n",
       "1                                                    CEC   \n",
       "2                                                    CEC   \n",
       "3                                                    CEC   \n",
       "5                                                    CEC   \n",
       "...                                                  ...   \n",
       "61918  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...   \n",
       "61920  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHH...   \n",
       "61921  CCCCHHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...   \n",
       "61922  CCCCCHHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...   \n",
       "61923  CCCCCCHHHHHHHHHHHHHHCCCCHHHHHHHCHHHHHHHHHHHHHH...   \n",
       "\n",
       "                                                    sst8  remove  len  \n",
       "0                                                    CBC       0    3  \n",
       "1                                                    CBC       0    3  \n",
       "2                                                    CBC       0    3  \n",
       "3                                                    CBC       0    3  \n",
       "5                                                    CBC       0    3  \n",
       "...                                                  ...     ...  ...  \n",
       "61918  CCCCCCCCCCCCCCCCCCCCCCCCCCCCSCCCCCHHHHHHHHHHHH...       0  100  \n",
       "61920  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCTTCCCHHHHHHHHHHHH...       0  100  \n",
       "61921  CCCCHHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...       0  100  \n",
       "61922  CCCCCHHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...       0  100  \n",
       "61923  CCCCCCHHHHHHHHHHHHHHTSSCHHHHHHHCHHHHHHHHHHHHHH...       0  100  \n",
       "\n",
       "[42386 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"len\"] = sample[\"seq\"].apply(len)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\"}\n",
    "        self.n_words = len(self.index2word)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in list(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang() \n",
    "\n",
    "    pairs = list(zip(lang1,lang2))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(f\"Sequence: {input_lang.n_words}\")\n",
    "    print(f\"Structure: {output_lang.n_words}\")\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in list(sentence)]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(input_lang,output_lang,pairs, max_len):\n",
    "\n",
    "    MAX_LENGTH = max_len\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    X = torch.tensor(input_ids, dtype=torch.long)\n",
    "    y = torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inpit_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = F.relu(x)\n",
    "        x , hidden = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim = -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "batch_size = 128\n",
    "hidden_size = 256\n",
    "n_epochs = 100\n",
    "\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(sample)*0.6)\n",
    "test_size = int(len(sample)*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sample[:train_size].sort_values(\"len\")\n",
    "test = sample[train_size:train_size+test_size].sort_values(\"len\")\n",
    "val = sample[train_size+test_size:]\n",
    "\n",
    "X_train = train[[\"seq\",\"len\"]]\n",
    "y_train = train[\"sst3\"]\n",
    "\n",
    "X_test = test[[\"seq\",\"len\"]]\n",
    "y_test = test[\"sst3\"]\n",
    "\n",
    "X_val = val[[\"seq\",\"len\"]]\n",
    "y_val = val[\"sst3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "Sequence: 22\n",
      "Structure: 4\n"
     ]
    }
   ],
   "source": [
    "pairs = list(zip(sample[\"seq\"],sample[\"sst3\"]))\n",
    "input_lang, output_lang, pairs = prepareData(sample[\"seq\"], sample[\"sst3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem that appeared with dynamic length approach was to define the weight of SOS token for the loss function. I wanted to keep using weights but it depends on the SOS padding token frequency which varies in the batch. \n",
    "\n",
    "I decided to calculate the average value of SOS frequency by iterating over prepared batches and collecting the results on the list. I used mean value as the SOS token frequency. Using weights like that may result in some inconsistency in the training but I decided to try it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(word2index, word2count, sos_freq):\n",
    "        \n",
    "    vocab = word2index\n",
    "    word_freq = word2count\n",
    "\n",
    "    vocab.update({\"SOS\":0})\n",
    "    word_freq.update({\"SOS\":int(sos_freq)})\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    weights = torch.zeros(vocab_size)\n",
    "\n",
    "    for word, idx in vocab.items():\n",
    "        weights[idx] = 1.0 / (word_freq[word]) \n",
    "        \n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = len(X_train) // batch_size\n",
    "\n",
    "sos_freq = []\n",
    "\n",
    "for batch in range(batches):\n",
    "    i = batch * batch_size\n",
    "    X_batch = X_train[i:i+batch_size]\n",
    "    y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "    MAX_LENGTH = (X_batch[\"len\"].max()+1)\n",
    "\n",
    "    pairs = list(zip(X_batch[\"seq\"],y_batch))\n",
    "    X,y = preprocess_dataset(input_lang, output_lang, pairs, MAX_LENGTH)\n",
    "    SOS_freq = (y.shape[0] * y.shape[1]) - torch.count_nonzero(y)\n",
    "    sos_freq.append(int(SOS_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = calculate_weights(output_lang.word2index, output_lang.word2count,int(np.mean(sos_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_lang.n_words, 64, hidden_size, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.7559848180925003, Test loss: 0.7850497961044312\n",
      "Epoch: 1, Train loss: 0.6660050371981631, Test loss: 0.7006310820579529\n",
      "Epoch: 2, Train loss: 0.6297945715863296, Test loss: 0.6682461500167847\n",
      "Epoch: 3, Train loss: 0.6084610615113769, Test loss: 0.6453531384468079\n",
      "Epoch: 4, Train loss: 0.5882109501446137, Test loss: 0.6343716382980347\n",
      "Epoch: 5, Train loss: 0.5686587784627471, Test loss: 0.6058722138404846\n",
      "Epoch: 6, Train loss: 0.5509921527571149, Test loss: 0.5952735543251038\n",
      "Epoch: 7, Train loss: 0.5364478793409135, Test loss: 0.5826229453086853\n",
      "Epoch: 8, Train loss: 0.5190130567008798, Test loss: 0.5543151497840881\n",
      "Epoch: 9, Train loss: 0.5033552073168032, Test loss: 0.5484493970870972\n",
      "Epoch: 10, Train loss: 0.49126525990890735, Test loss: 0.542206346988678\n",
      "Epoch: 11, Train loss: 0.4782143950161308, Test loss: 0.5290931463241577\n",
      "Epoch: 12, Train loss: 0.4682964753321927, Test loss: 0.5224770307540894\n",
      "Epoch: 13, Train loss: 0.45489007008798193, Test loss: 0.5172946453094482\n",
      "Epoch: 14, Train loss: 0.44594461884763503, Test loss: 0.5025215744972229\n",
      "Epoch: 15, Train loss: 0.4442652089126182, Test loss: 0.5058407783508301\n",
      "Epoch: 16, Train loss: 0.4368434140477518, Test loss: 0.4944537878036499\n",
      "Epoch: 17, Train loss: 0.4292319660837, Test loss: 0.49531498551368713\n",
      "Epoch: 18, Train loss: 0.420829389432464, Test loss: 0.480694979429245\n",
      "Epoch: 19, Train loss: 0.4232225579896359, Test loss: 0.482989102602005\n",
      "Epoch: 20, Train loss: 0.41724647205285353, Test loss: 0.48392564058303833\n",
      "Epoch: 21, Train loss: 0.4121332319848465, Test loss: 0.4795505404472351\n",
      "Epoch: 22, Train loss: 0.40792098129638515, Test loss: 0.48443830013275146\n"
     ]
    }
   ],
   "source": [
    "test_loss_array = []\n",
    "best_result = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    total_loss = 0\n",
    "    batches = len(X_train) // batch_size\n",
    "\n",
    "    for batch in range(batches):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        i = batch * batch_size\n",
    "\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        MAX_LENGTH = (X_batch[\"len\"].max()+1)\n",
    "\n",
    "        pairs = list(zip(X_batch[\"seq\"],y_batch))\n",
    "        X,y = preprocess_dataset(input_lang, output_lang, pairs, MAX_LENGTH)\n",
    "\n",
    "        output = model(X)\n",
    "\n",
    "        loss = loss_fn(output.permute(1,2,0), y.permute(1,0))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "\n",
    "    batches = len(X_test) // batch_size\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        for batch in range(batches):\n",
    "            i = batch * batch_size\n",
    "\n",
    "            X_batch = X_test[i:i+batch_size]\n",
    "            y_batch = y_test[i:i+batch_size]\n",
    "\n",
    "            MAX_LENGTH = (X_batch[\"len\"].max()+1)\n",
    "\n",
    "            pairs = list(zip(X_batch[\"seq\"],y_batch))\n",
    "            X,y = preprocess_dataset(input_lang, output_lang, pairs, MAX_LENGTH)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = loss_fn(output.permute(1,2,0), y.permute(1,0))\n",
    "\n",
    "            test_loss+=loss\n",
    "\n",
    "    loss = total_loss / (len(X_train) // batch_size)\n",
    "    loss_test = test_loss / (len(y_test) // batch_size)\n",
    "\n",
    "    test_loss_array.append(loss_test)\n",
    "\n",
    "    if loss_test < best_result:\n",
    "        torch.save(model.state_dict(), \"../Models/lstm_var_len.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train loss: {loss}, Test loss: {loss_test}\")\n",
    "\n",
    "    if len(test_loss_array)>patience+1:\n",
    "        if not (any(x > (test_loss_array[-1]+0.015) for x in test_loss_array[len(test_loss_array)-patience-1:-1])):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../Models/lstm_var_len.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embed): Embedding(22, 64)\n",
       "  (lstm): LSTM(64, 256, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../Models/lstm_var_len.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = (X_val[\"len\"].max()+1)\n",
    "pairs = list(zip(X_val[\"seq\"],y_val))\n",
    "X,y = preprocess_dataset(input_lang, output_lang, pairs, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCHHCCCHCCCCCCCCEEEEECCCCCCCCCCEEHHHHHCHCCCEECHHHHHCCHHCCCCCCCCCEEECCCCCCCHCHHHHHHHHHHCC', 'CCCCHCCCCCCCCEEEECCCEECECCEEECCEECCCEECEEEECCCEECCCCCCECCCCCHHHCCCCCCCCECEEEEEEEC', 'CCCCCCCCCCHHHHHHHHCCCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCCCCCCCCCCCC', 'CCCCCCCCHHHHHHCCCHCCCCCCCCCCCHCCCCHHCHHHHCCCCHHHHCCCCHHHHCCCCCCCCCC']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    outputs_pred = model(X)\n",
    "\n",
    "    _, topi = outputs_pred.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "\n",
    "    pred = []\n",
    "    for idx in decoded_ids:\n",
    "        decoded_structure = []\n",
    "        for id in idx:\n",
    "            if id.item() == SOS_token:\n",
    "                break\n",
    "            decoded_structure.append(output_lang.index2word[id.item()])\n",
    "        pred.append(\"\".join(decoded_structure))\n",
    "    \n",
    "    print(pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCHHHHHHHHCCC', 'CCCHHHHHHHCHHHHCCCCCCCCCCCCCCCCCCCCECCCCHHHCCCCHHHHHCCCECCHHHHHHHHHCHHHHCCCCCCCCCCCCCHHHHHHHHHHHHCCC', 'CHHHHHHHCCCCCCCCEECCCCCEEECCCCCCECCCEEECCEEEEEEECCCCCCCHHHHHHHHHHHCCCCCCCCCCCEEEC', 'CCCCCCCECEHHHHHHHHCCECEHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCC', 'CCCCCCCCHHHHCCCCHHHCCCCCCCCCHHHHCCHHHHHHHHCCCHHHHCCCCHHHHHHHHHHHCCC']\n"
     ]
    }
   ],
   "source": [
    "target=[]\n",
    "for idx in y:\n",
    "    decoded_structure = []\n",
    "    for id in idx:\n",
    "        if id.item() == SOS_token:\n",
    "            break\n",
    "        decoded_structure.append(output_lang.index2word[id.item()])\n",
    "    target.append(\"\".join(decoded_structure))\n",
    "\n",
    "print(target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level accuracy: 77.74467810611536%\n",
      "Character-level f1: 66.05239671982044%\n",
      "Exact match: 4.246284501061571%\n"
     ]
    }
   ],
   "source": [
    "def char_level_metrics(predictions, targets):\n",
    "    accuracy = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    for pred, target in zip(list(predictions), list(targets)):\n",
    "        if len(pred)<len(target):\n",
    "            pred = pred + (\"$\" * (len(target)-len(pred)))\n",
    "        if len(pred)>len(target):\n",
    "            target = target + (\"$\" * (len(pred)-len(target)))\n",
    "\n",
    "        accuracy += accuracy_score(list(pred),list(target))\n",
    "        f1 += f1_score(list(pred),list(target), average=\"macro\")\n",
    "\n",
    "    return accuracy/len(predictions), f1/len(predictions)\n",
    "\n",
    "ac, f1 = char_level_metrics(pred, target)\n",
    "\n",
    "print(f'Character-level accuracy: {ac*100}%')\n",
    "print(f'Character-level f1: {f1*100}%')\n",
    "print(f'Exact match: {accuracy_score(pred,target)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
