{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = pd.read_csv(\"./Data/2018-06-06-ss.cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>chain_code</th>\n",
       "      <th>seq</th>\n",
       "      <th>sst8</th>\n",
       "      <th>sst3</th>\n",
       "      <th>len</th>\n",
       "      <th>has_nonstd_aa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1A30</td>\n",
       "      <td>C</td>\n",
       "      <td>EDL</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1B05</td>\n",
       "      <td>B</td>\n",
       "      <td>KCK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1B0H</td>\n",
       "      <td>B</td>\n",
       "      <td>KAK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1B1H</td>\n",
       "      <td>B</td>\n",
       "      <td>KFK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1B2H</td>\n",
       "      <td>B</td>\n",
       "      <td>KAK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393727</th>\n",
       "      <td>4UWE</td>\n",
       "      <td>D</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCBTTCEEEEEEEEEETTEEEEEEEECCCSSCCB...</td>\n",
       "      <td>CCCCCCCCCCCCCCECCCEEEEEEEEEECCEEEEEEEECCCCCCCE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393728</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>A</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393729</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>B</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393730</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>C</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393731</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>D</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393732 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pdb_id chain_code                                                seq  \\\n",
       "0        1A30          C                                                EDL   \n",
       "1        1B05          B                                                KCK   \n",
       "2        1B0H          B                                                KAK   \n",
       "3        1B1H          B                                                KFK   \n",
       "4        1B2H          B                                                KAK   \n",
       "...       ...        ...                                                ...   \n",
       "393727   4UWE          D  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393728   5J8V          A  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393729   5J8V          B  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393730   5J8V          C  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393731   5J8V          D  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "\n",
       "                                                     sst8  \\\n",
       "0                                                     CBC   \n",
       "1                                                     CBC   \n",
       "2                                                     CBC   \n",
       "3                                                     CBC   \n",
       "4                                                     CBC   \n",
       "...                                                   ...   \n",
       "393727  CCCCCCCCCCCCCCBTTCEEEEEEEEEETTEEEEEEEECCCSSCCB...   \n",
       "393728  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393729  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393730  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393731  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "\n",
       "                                                     sst3   len  has_nonstd_aa  \n",
       "0                                                     CEC     3          False  \n",
       "1                                                     CEC     3          False  \n",
       "2                                                     CEC     3          False  \n",
       "3                                                     CEC     3          False  \n",
       "4                                                     CEC     3          False  \n",
       "...                                                   ...   ...            ...  \n",
       "393727  CCCCCCCCCCCCCCECCCEEEEEEEEEECCEEEEEEEECCCCCCCE...  5037           True  \n",
       "393728  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393729  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393730  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393731  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "\n",
       "[393732 rows x 7 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with sampling the dataset: \n",
    "- selected sequences no longer than 20 amino acids (in this architecture I used shorter sequences than in LSTM because of the resources needed to train on longer sequences)\n",
    "- selected the important columns\n",
    "- deduplicated dataset\n",
    "- removed sequences that were only composed of \"*\" sign - which indicated nonstandard amino acids (B, O, U, X, or Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = proteins[\n",
    "    (proteins[\"len\"]>=1) &\n",
    "    (proteins[\"len\"]<=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[[\"seq\",\"sst3\",\"sst8\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"len\"] = sample[\"seq\"].apply(len)\n",
    "sample = sample.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the seq2seq tutorial (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) I created Lang class that will store the input \"language\" - in my case amino acid sequence and the output \"language\" - three-state (Q3) secondary structure\n",
    "\n",
    "The sequences and structures are characterised by their own set of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in list(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "Sequence: 23\n",
      "Structure: 5\n",
      "('DIFGAIWPLALGALKNLIK*', 'CCCCCHHHHHHHHHHHHHCC')\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang() \n",
    "\n",
    "    pairs = list(zip(lang1,lang2))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(f\"Sequence: {input_lang.n_words}\")\n",
    "    print(f\"Structure: {output_lang.n_words}\")\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(sample[\"seq\"], sample[\"sst3\"])\n",
    "\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 53524, 'H': 12860, 'E': 7831}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'H': 3, 'E': 4}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = sample[\"len\"].max()+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in list(sentence)]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "n = len(pairs)\n",
    "input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_lang, inp)\n",
    "    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "    inp_ids.append(EOS_token)\n",
    "    tgt_ids.append(EOS_token)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(input_ids)*0.6)\n",
    "test_size = int(len(input_ids)*0.2)\n",
    "\n",
    "X = input_ids\n",
    "y = target_ids\n",
    "\n",
    "X_train = torch.tensor(X[:train_size], dtype=torch.long)\n",
    "y_train = torch.tensor(y[:train_size],dtype=torch.long)\n",
    "\n",
    "X_test = torch.tensor(X[train_size:train_size+test_size],dtype=torch.long)\n",
    "y_test = torch.tensor(y[train_size:train_size+test_size],dtype=torch.long)\n",
    "\n",
    "X_val = torch.tensor(X[train_size+test_size:],dtype=torch.long)\n",
    "y_val = torch.tensor(y[train_size+test_size:],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to calculate weights of the output characters, and use them later in the loss function. The frequencies of the characters in the structure vary a lot with C letter being the most frequent. I wanted to avoid the situation where model learns only to output the majority group, so I calculated the reverse probability of the word frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_freq = (torch.tensor(y, dtype=torch.long).shape[0] * torch.tensor(y, dtype=torch.long).shape[1]) - torch.count_nonzero(torch.tensor(y, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0468, 0.4058, 0.0456, 0.1899, 0.3119])\n"
     ]
    }
   ],
   "source": [
    "vocab = output_lang.word2index\n",
    "word_freq = output_lang.word2count\n",
    "\n",
    "vocab.update({\"SOS\":0,\"EOS\":1})\n",
    "word_freq.update({\"SOS\":int(SOS_freq),\"EOS\":len(sample)})\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "weights = torch.zeros(vocab_size)\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    weights[idx] = 1.0 / (word_freq[word]) \n",
    "    \n",
    "weights = weights / weights.sum()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I based the model on seq2seq tutorial (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
    "\n",
    "I tested the result with the presented architecture as well as using the decoder without attention mechanism. The results were better with attention. Then I used bidirectional encoder which led to better results. I also decided to add embedding size parameter in the decoder to be able to adjust its size independently from hidden size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True, num_layers=1)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "\n",
    "        hidden = torch.sum(hidden,dim=0).unsqueeze(dim=0)\n",
    "        output = torch.chunk(output, 2 , dim = 2)[0] + torch.chunk(output, 2 , dim = 2)[1]\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_dim+hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, decoder_input, hidden, encoder_outputs):\n",
    "        output = self.embedding(decoder_input)\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((output, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experimented with learning rate, decided to use large batch size to speed up learning process, because the output was not highly dimensional I decided to use small hidden size and embedding.\n",
    "\n",
    "Finally in the training loop I implemented early stopping and saving best model, I wanted to be able to optimise the number of epochs and prevent overfitting, so if during the training the train loss is decreasing but test loss is rising I stop the training, and save the model that was having the lowest test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "batch_size = 128\n",
    "hidden_size = 32\n",
    "embedding_size = 32\n",
    "n_epochs = 100\n",
    "\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(X_train,y_train)), batch_size=batch_size)\n",
    "test_loader = DataLoader(list(zip(X_test,y_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, embedding_size, output_lang.n_words)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.325981765985489, Test loss: 1.0632933378219604\n",
      "Epoch: 1, Train loss: 0.7983005387442452, Test loss: 0.704858660697937\n",
      "Epoch: 2, Train loss: 0.5554359619106565, Test loss: 0.5345368981361389\n",
      "Epoch: 3, Train loss: 0.4426747815949576, Test loss: 0.4648110568523407\n",
      "Epoch: 4, Train loss: 0.3920128984110696, Test loss: 0.41604119539260864\n",
      "Epoch: 5, Train loss: 0.3603114549602781, Test loss: 0.38830944895744324\n",
      "Epoch: 6, Train loss: 0.34307172362293514, Test loss: 0.37202373147010803\n",
      "Epoch: 7, Train loss: 0.33369557027305874, Test loss: 0.36122068762779236\n",
      "Epoch: 8, Train loss: 0.3210261049015181, Test loss: 0.3555908203125\n",
      "Epoch: 9, Train loss: 0.3136281041162355, Test loss: 0.34400221705436707\n",
      "Epoch: 10, Train loss: 0.30405750977141516, Test loss: 0.3403071165084839\n",
      "Epoch: 11, Train loss: 0.3226709280695234, Test loss: 0.3445257246494293\n",
      "Epoch: 12, Train loss: 0.3008771411010197, Test loss: 0.33709049224853516\n",
      "Epoch: 13, Train loss: 0.29378327088696615, Test loss: 0.3268061578273773\n",
      "Epoch: 14, Train loss: 0.2888613609330995, Test loss: 0.32368046045303345\n",
      "Epoch: 15, Train loss: 0.2889133489557675, Test loss: 0.3274241089820862\n",
      "Epoch: 16, Train loss: 0.2854823278529303, Test loss: 0.32316625118255615\n",
      "Epoch: 17, Train loss: 0.28315176921231405, Test loss: 0.32466110587120056\n",
      "Epoch: 18, Train loss: 0.28082708535449846, Test loss: 0.31502485275268555\n",
      "Epoch: 19, Train loss: 0.27706671985132353, Test loss: 0.31591564416885376\n",
      "Epoch: 20, Train loss: 0.2755127857838358, Test loss: 0.31116729974746704\n",
      "Epoch: 21, Train loss: 0.2733310205595834, Test loss: 0.3089085817337036\n",
      "Epoch: 22, Train loss: 0.275595348328352, Test loss: 0.31236031651496887\n",
      "Epoch: 23, Train loss: 0.4343397915363312, Test loss: 0.37976473569869995\n"
     ]
    }
   ],
   "source": [
    "best_result = np.inf\n",
    "test_loss_array = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(X_batch)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, y_batch)\n",
    "\n",
    "        loss = loss_fn(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            y_batch.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  \n",
    "            \n",
    "            for X_batch, y_batch in test_loader:\n",
    "\n",
    "                encoder_outputs, encoder_hidden = encoder(X_batch)\n",
    "                decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, y_batch)\n",
    "                t_loss = loss_fn(decoder_outputs.view(-1, decoder_outputs.size(-1)),y_batch.view(-1))\n",
    "\n",
    "                test_loss+=t_loss\n",
    "\n",
    "    loss = total_loss / (len(X_train) // batch_size)\n",
    "    loss_test = test_loss / (len(y_test) // batch_size)\n",
    "\n",
    "    test_loss_array.append(loss_test)\n",
    "\n",
    "    if loss_test < best_result:\n",
    "        torch.save(encoder.state_dict(), \"./encoder.pth\")\n",
    "        torch.save(decoder.state_dict(), \"./decoder.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train loss: {loss}, Test loss: {loss_test}\")\n",
    "\n",
    "    if len(test_loss_array)>patience+1:\n",
    "        if not (any(x > test_loss_array[-1] for x in test_loss_array[len(test_loss_array)-patience-1:-1])):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I loaded the best model and used it for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(\"./encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"./decoder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEE', 'CEEEEE', 'CEEEEEEEE', 'CCCCCCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCCC', 'CEEEEEE', 'CEEEE', 'CCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEE', 'CEEEEE', 'CCCCCCCCCCC', 'CCHHHHHHHHHHHHH', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCC', 'CCHHHHHHHHHHHHHHHHH', 'CCCCCCCCCC', 'CEEEE', 'CEEEEE', 'CCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CEEEEEEE', 'CEEE', 'CCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCCC', 'CEEEEEE', 'CCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCC', 'CEE', 'CEEEEEE', 'CEEEEE', 'CEEEEE', 'CCCCCCCCCCCC', 'CEEEEEEEE', 'CEEEEEE', 'CEEEEE', 'CCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCHHHHHHHHHHHH', 'CEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEE', 'CCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCCC', 'CEEEEEEEEE', 'CEEEEEE', 'CCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCC', 'CHHHHHHHHHHHHHHHH', 'CEEEE', 'CEEEE', 'CEEEEEE', 'CEEEEE', 'CCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEE', 'CEEEEE', 'CEEEEE', 'CEEEEEEEE', 'CEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CCHHHHHHHHHHHHHH', 'CCHHHHHHHHHHHHHH', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CEEEE', 'CEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEE', 'CEE', 'CEEE', 'CCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCHHHHHHHHHHHHHHHHH', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCC', 'CCCCCCCCCCCC', 'CEEEEE', 'CCHHHHHHHHHHHHHHHH', 'CCCCCCCCCC', 'CEEE', 'CCCCCCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CHHHHHHHHHHHHHHHHHHH', 'CEEEEE', 'CCCCCCCCCC', 'CEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEE', 'CEEEEE', 'CEEEEE', 'CCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEEEEEE', 'CCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CCHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEE', 'CEEEEEEE', 'CCCCCCCCC', 'CEEEEEEE', 'CEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHH', 'CCHHHHHHHHH', 'CHHHHHHHHHHHHHHHHH', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CEEEEEEEE', 'CEEEEEE', 'CEEEEEEEE', 'CEEEEEEE', 'CCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEE', 'CHHHHHHHHHHHHHHHHHH', 'CEEEEEE', 'CCHHHHHHHHHHH', 'CEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCC', 'CCHHHHHHHHHHH', 'CCCCCCCCCC', 'CCCCCCCCC', 'CCHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCC', 'CHHHHHHHHHHHHHHHHHH', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CEEEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHHH', 'CEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCC', 'CEE', 'CEEEEEEEE', 'CCHHHHHHHHHH', 'CEEEEE', 'CCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CEEEEE', 'CEEEEEEE', 'CEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEEEEE', 'CEE', 'CCCCCCCC', 'CCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEE', 'CCHHHHHHHHHHHHHH', 'CEEEEEEEE', 'CEEEEEEEE', 'CEEEEEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEEEEE', 'CEE', 'CCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEEEE', 'CEEEEEE', 'CEEEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEEEEE', 'CEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CEEEEEE', 'CEEEEE', 'CCCCCCCCCC', 'CCCCCCCC', 'CEEEEE', 'CCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CEEEEEEEE', 'CCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEEEEEEE', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCCCC', 'CEEEE', 'CEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCC', 'CEEEEEEEE', 'CEEEEEEEE', 'CEEEEE', 'CEEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCCC', 'CEE', 'CCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEEE', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCC', 'CEEEE', 'CEEE', 'CEEEEE', 'CEEEEE', 'CCCCCCCCCCCCCCC', 'CEEEEEEE', 'CEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEE', 'CCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCHHHHHHHHHHHHH', 'CEEE', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCCC', 'CCHHHHHHHHHHHHHH', 'CCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEEEEE', 'CCHHHHHHHHHHHHHHHHH', 'CCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEE', 'CEEEEE', 'CCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CEEEE', 'CCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCC', 'CEEEE', 'CEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCC', 'CCHHHHHHHHHHHHHH', 'CEEE', 'CEEEEEEEE', 'CEEEEEEEE', 'CEEEEE', 'CHHHHHHHHHHHHHHH', 'CHHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCC', 'CEEEEE', 'CCCCCCCC', 'CCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCC', 'CCHHHHHHHHHHHHH', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CEEEE', 'CEEEEEEE', 'CEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCCC', 'CEEEEEE', 'CEEEEEE', 'CCCCCCCCCC', 'CHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEEE', 'CEEEEE', 'CCCCCCCCCC', 'CEEEEEE', 'CEEEEE', 'CCHHHHHHHHHHH', 'CEEEEEEEE', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEE', 'CEEEEEEEE', 'CCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEE', 'CHHHHHHHHHHHHHHHH', 'CEEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCC', 'CEEEEEE', 'CCHHHHHHHHHHH', 'CEEEEEEEE', 'CEEEE', 'CCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCC', 'CEEEE', 'CCCCCCCCCC', 'CCCCCCC', 'CEEEEEEEE', 'CCHHHHHHHHHHHHH', 'CEEEEEEEEE', 'CEEEEEEE', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCHHHHHHHHHHHHH', 'CEEEEEEEEE', 'CCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CCHHHHHHHHHHH', 'CEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEE', 'CCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCC', 'CEEEEEEEE', 'CEEEEEE', 'CEEEEEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEE', 'CCCCCCCC', 'CEEEEE', 'CEEEE', 'CCCCCCCCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CEEEEEE', 'CCCCCCCCCC', 'CEEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCCCCCC', 'CEEEEE', 'CEEEEEEE', 'CCHHHHHHHHHHHHHH', 'CEEEEEEE', 'CCCCCCCC', 'CCHHHHHHHHHHHH', 'CCCCCCCCCCCC', 'CEEE', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEE', 'CEEEEEEE', 'CCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CCHHHHHHHHHHHHHH', 'CEEEE', 'CEEEEEE', 'CCCCCCCCCCCCCC', 'CEEEEEEEE', 'CEEEEE', 'CEEEEE', 'CEEEEEEEE', 'CCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCC', 'CCHHHHHHHHHHHHHHHHH', 'CCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEE', 'CCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCHHHHHHHHHHHH', 'CCCCCCCCCC', 'CCHHHHHHHHH', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEE', 'CEEEEEE', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEE', 'CEEEEEE', 'CCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCC', 'CEEEEE', 'CEEEE', 'CEEE', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCCCC', 'CEEEE', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEE', 'CEEEEEEE', 'CCHHHHHHHHHHHHHHH', 'CCCCCCCCCCCC', 'CEEEEEEE', 'CCHHHHHHHHHHHH', 'CEEEEE', 'CEEEE', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCC', 'CEEEEEE', 'CEEEEEE', 'CEEEEEEEEE', 'CEEEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEEEE', 'CHHHHHHHHHHHHHHH', 'CEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCC', 'CEEEEE', 'CCCCCCCCCC', 'CEEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEE', 'CCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEEE', 'CCHHHHHHHHHHHHHH', 'CCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCC', 'CCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCC', 'CEEEEEE', 'CEEEE', 'CCHHHHHHHHHHHHHH', 'CEEEEEEEE', 'CEEEEEEEE', 'CEEEEEEE', 'CCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CEE', 'CCCCCCCCCCCCC', 'CCHHHHHHHHHHHHHH', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCC', 'CCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCHHHHHHHHHHHH', 'CCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCCC', 'CEEEEEEEEEEE', 'CEEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCC', 'CEEEEEEE', 'CEEE', 'CCCCCCC', 'CCCCCCCCCC', 'CEE', 'CEEEEE', 'CEEE', 'CCCCCCCCCCCCCC', 'CEEEE', 'CEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCC', 'CCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCCCCCCCC', 'CEEEEEE', 'CEEEEE', 'CCCCCCCC', 'CCCCCCCC', 'CHHHHHHHHHHHHHHHHHH', 'CEEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEEEEEE', 'CEEEEEEE', 'CCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CEE', 'CCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCC', 'CCHHHHHHHHHHHHHHHHHH', 'CEEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CEEEE', 'CHHHHHHHHHHHHHH', 'CCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHH', 'CCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCC', 'CEEEEE', 'CCCCCCCCCC', 'CEEEEEEEEE', 'CCCCCCCCCCC', 'CEEEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CEEEEE', 'CEEEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCHHHHHHHHHHHHH', 'CEEEEE', 'CCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CEEEEE', 'CEEEEEEEE', 'CEEEEE', 'CEEE', 'CCHHHHHHHHHHHHHH', 'CCCCCCCC', 'CEEEEEE', 'CCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEE', 'CEEEEEEE', 'CEEEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCC', 'CEEEEEEEE', 'CCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEE', 'CEEEEEEE', 'CCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCC', 'CCHHHHHHHHHHHHH', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEE', 'CEEEEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCCCC', 'CEEEEEEE', 'CCHHHHHHHHHHHHHHHH', 'CEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEEEE', 'CEEEEE', 'CEEEEE', 'CCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CEEEE', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEE', 'CEEEEEEEEE', 'CCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEE', 'CEEEE', 'CCCCCCCC', 'CEEEEE', 'CEEEEE', 'CEEEEE', 'CEEEEEE', 'CEEEEEEE', 'CEEEEE', 'CCHHHHHHHHHHHHHH', 'CEEE', 'CEEEEEEE', 'CCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCC', 'CEEEEEEE', 'CEEEEEEEE', 'CEEEEE', 'CCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEE', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCC', 'CEEEE', 'CCHHHHHHHHHHHHHH', 'CCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEEEEE', 'CEEEEE', 'CCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEEE', 'CEEEE', 'CEEEEE', 'CEEEEEEEE', 'CEEEEEEE', 'CCCCCCCC', 'CEEEEEEEEE', 'CEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEE', 'CEEEEE', 'CEE', 'CEEEEEEEE', 'CCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CEEE', 'CCCCCCCCCCCCCCC', 'CCHHHHHHHHHHHHHHHH', 'CEEEEEEE', 'CEEEEEEE', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CEEEEEEEE', 'CEEEEEEE', 'CEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCC', 'CEEEEEE', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEEE', 'CCHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCC', 'CEEEEE', 'CEEEEEEEE', 'CEEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCC', 'CCHHHHHHHHHHHHH', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEE', 'CCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CCEEEEEEEEE', 'CEEEEEE', 'CEEEEEEEEE', 'CEEEEEEE', 'CEEEEEEE', 'CCHHHHHHHHHHHHH', 'CHHHHHHHHHHHHHH', 'CCCCCCCCCCCCC', 'CCHHHHHHHHHHHHHHH', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEEEEE', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CEEEE', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CEEEE', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCCCC', 'CEEEEEEE', 'CCCCCCCCC', 'CEEEEE', 'CEEEEEEEEEE', 'CHHHHHHHHHHHHHHHHH', 'CEEEEEEEE', 'CCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCC', 'CHHHHHHHHHHHHHHHHH', 'CCCCCCCCCCCCCCCC', 'CEEEEEEEE', 'CCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEEE', 'CEEEEEE', 'CEEEEEEEE', 'CEEEE', 'CCHHHHHHHHHHHHHHHHH']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(X_val)\n",
    "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "\n",
    "    pred = []\n",
    "    for idx in decoded_ids:\n",
    "        decoded_structure = []\n",
    "        for id in idx:\n",
    "            if id.item() == EOS_token or id.item() == SOS_token:\n",
    "                break\n",
    "            decoded_structure.append(output_lang.index2word[id.item()])\n",
    "        pred.append(\"\".join(decoded_structure))\n",
    "    \n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCEECCCCCCCC', 'CEEECCHHHCC', 'CCCCCCCCCC', 'CCCEECC', 'CCCCCCCCCC', 'CCCCHHHHCCCCCCCC', 'CCCCCCCCCCCC', 'CCCHHHHHHHCC', 'CCCCCCCC', 'CCCCC', 'CCCCHHHHHC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCHHHHHCCCCCCCC', 'CCCCCCCECC', 'CCEEEEEHHHHHHHCC', 'CCCCCCC', 'CCCCCCCCCCCC', 'CCCCCECCCCCCCCC', 'CCCHHHHHHHCCC', 'CCCCCCCCCCCCHHHCCC', 'CEEC', 'CECCCC', 'CCCHHHHHHHHHHCC', 'CCCCCCCCCCCCCEEEEC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCECCCCCCCECCC', 'CCCCCCCEEECCCEEECCCC', 'CECCCCCCCCCCC', 'CEEEC', 'CCEEEC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCHHHHHCCC', 'CCCCCCCCC', 'CCEC', 'CCCCCCCCHHHCCCC', 'CEECCCC', 'CCCCHHHHHHCC', 'CECCCCCCC', 'CCCCCCCCCC', 'CCCCCCCECCC', 'CHHHHHHHHHHHHHHHCCCC', 'CCEEEECCCCCCC', 'CCHHHHHHHCCC', 'CEC', 'CCCCCCEC', 'CEEECCC', 'CCCEEEC', 'CEEEEEECCCEEEEEC', 'CCCCCCCCCC', 'CCCCCCCC', 'CCCCCCC', 'CCCCECCCCC', 'CCCCCCCCC', 'CCCCCCECCCCCCCC', 'CECCCEECCCCCCCCCEEC', 'CCCCCCCCC', 'CCECECC', 'CCCCCCCCCCCCCCCC', 'CHHHHHHCCCC', 'CCCECCCCCCCCEC', 'CCCECC', 'CCCHHHHHHHHHCCCC', 'CHHHHHHHHC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCHHHHHHHCCCC', 'CCCHHHHHHHHHHCCC', 'CCCCCCC', 'CCECCCCCC', 'CCCCCCCCCCEECCCC', 'CCCCCCCCCCC', 'CCECCECC', 'CCCCCCCCCCCCCCC', 'CCCEECCC', 'CCCCCCCCCCCCCCCCC', 'CEECCCCC', 'CCCCEECCCEECC', 'CCCCCCCCCCC', 'CCEECCCC', 'CCECCECC', 'CECCCCC', 'CCCCCCCCCCCEECCCCC', 'CCCHHHHHHHHCCC', 'CCHHHHHHCCCC', 'CCEEEECCCCCCCCCC', 'CCEEEEC', 'CCCCCCCCECCCCCC', 'CCCCCCCCC', 'CCHHHHHHHHCCCCCC', 'CCCCCCCC', 'CCECCECC', 'CCECCCCCC', 'CCCCHHHHHHHHHHCC', 'CCCCCCCC', 'CCCEECCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCECCC', 'CCCCCCCCHHHHHCCCCCC', 'CCECC', 'CECCC', 'CCCCCCCC', 'CCHHHHC', 'CCCCCCEECCC', 'CHHHHHHHHHHHHHHHC', 'CCCCCCCCCCCCCCCCC', 'CECCCC', 'CCCCCCC', 'CCCCEEC', 'CCCCCCCCCC', 'CCCCEEEEC', 'CCCCEECCCC', 'CHHHHHHHHHHHCCC', 'CCCCCCCCCCCCCCCCCCC', 'CHHHHHHHHCC', 'CCCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCCC', 'CCCCCHHHHHHHHHHHC', 'CCEEEEEEECCCEECCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCHHHHHHHCCCCCC', 'CCCEEEECCCCCC', 'CCECCCCCCCCCCCCECCCC', 'CCCCCCHHHHHC', 'CCCCCC', 'CEEEEC', 'CCCEEEEECCEEEEECCC', 'CCEEEECCEEEECC', 'CCCCCCCC', 'CEC', 'CCEC', 'CCCCCHHHHHCCCCCCC', 'CCCCCCEEEC', 'CCCCCCEECCCEECCCCCCC', 'CCECCCCCCCCCC', 'CCCCCCCCCCCCCCCCCC', 'CCHHHHHHHHHHCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCC', 'CCCCCCCEEECCCEEECCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCC', 'CCCCECCHHHHHHHHC', 'CHHHCC', 'CCCCCCCCCEEEEEEECCCC', 'CCCCCCECCCCCCC', 'CCCC', 'CCCCCECCCCCCCCCCC', 'CEEEC', 'CCCCCHHHHHHHCCCCC', 'CCCCCCECCCCCC', 'CCCCHHHHEC', 'CECCCCCCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCEEEECCCCEEEEECCCC', 'CHHHHHHCCCCCCCCCC', 'CCCCCCCCCCC', 'CCECCCCCCCECCC', 'CCCCHHHHHHHHHHHHHHCC', 'CCCCCHHHCHHHHHHCC', 'CCCCCCCCCHHHCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCHHHHCCCHHHC', 'CCCCCCCCCCC', 'CCHHHHHHHCCCC', 'CCCCCCCCCCCHHHHCCCCC', 'CCEECC', 'CCCCCCCCEEECC', 'CCEECC', 'CEECCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCECCECC', 'CCECCCEC', 'CCEEEEC', 'CCCCCCC', 'CCCEECCEECCCCC', 'CCCCCCCCCCC', 'CCCCCCEEECEEEEC', 'CCCEEECC', 'CCCCHHHHC', 'CHHHHHHHHHHHHHC', 'CCCCCCECCCCC', 'CCCCCCCCEECCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCCCCC', 'CCCCCCC', 'CCEEEEECCEEEEEEC', 'CCCCCCCCCCCC', 'CCHHHHHHHHHHHCC', 'CCCCCHHHHHHHCCCCCCCC', 'CHHHHHHHHHHHHHHCCCC', 'CCCECEEECCECCCCCCCC', 'CCECC', 'CCCCCCCCC', 'CCCCCCCECCC', 'CHHHHHCCC', 'CEEEEEC', 'CECCCHHHCCC', 'CCHHHHHHHHHHHHHHCC', 'CCCCCEECCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCECCCCC', 'CCCCECCCCCCC', 'CCCCCCHHHHHHHHHC', 'CHHHHHHHHHHHHC', 'CCCCCCCCCCCCCCCCCC', 'CCEEEEECCCCEEEEECC', 'CCCCCCECCECCC', 'CCCCCCCEEECCCEECCCCC', 'CEEEEC', 'CCHHHHHHHHHHHHCCCC', 'CCCCCCCCCC', 'CCCCCEC', 'CCCCCCCCCC', 'CEECCCCCC', 'CCHHHHCCCCC', 'CCEECEC', 'CCHHHHHHHHHCCCC', 'CCCCECCC', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCECCC', 'CCCCCCCCCCCCCC', 'CCCCECCCCCCCC', 'CECCEC', 'CCCCCEECCCCHHHHCCCCC', 'CCECCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCHHHCC', 'CCCCHHHHHHCCC', 'CCCCCCCEEECCEEEC', 'CCECCCCCCCCEC', 'CEEEECCCCCCC', 'CCCCCCHHHCCCCCHHHCC', 'CCCCCCCHHHHHHCCCCCCC', 'CCCCCCCCCCCC', 'CCEEECCCCCCECCCC', 'CCCCCCCCCCC', 'CCCHHHHHHHHHCC', 'CHHHHHHHHHHHHHHHHC', 'CCCCCCEEEC', 'CCCCCHHHHCCCCC', 'CCCCCCCCCCCCCEC', 'CCECCECC', 'CCHHHHHHHHHHHHCCCC', 'CCCEECCCCCCCCCCEECCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCHHHHHCCCCCC', 'CCCCCCC', 'CCCCCCCCCCCCCCCCCC', 'CCHHHHCCCCC', 'CCCHHHHHHHHHCC', 'CCCCCCCCCCCCC', 'CEEEECCCCCCEC', 'CECCC', 'CHHHHHHHCCCCCCCCCEEC', 'CCCHHHHHHHHHECC', 'CCCCCCCCCCCCC', 'CCCCCCCCCHHHCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCEEEC', 'CHHHHHHHHHHCCCCCCCCC', 'CCHHHHHHHHCC', 'CCCCCHHHHHCCCC', 'CCCCCCCHHHCCCCC', 'CCCCCCCCCHHHHHCCCCCC', 'CCCCCCCCCCC', 'CECECCEECC', 'CCHHHHCCHHHHCCHHHCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCEEC', 'CCEECCCCECCECCCECCC', 'CCCCCEECCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCCC', 'CCEC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CEEECC', 'CHHHHHHHCCC', 'CCCEEEEEC', 'CCCCCCHHHC', 'CCEEECC', 'CCCCCCCCC', 'CCCCCCCCC', 'CEEECCCCEEEC', 'CCCCCCCCCCCCCC', 'CHHHHHHHHHHHHHC', 'CCCCCCCEEC', 'CCEC', 'CCHHHHHHHCC', 'CCCCCCCCCCC', 'CCCCCHHHHCC', 'CCCCCHHHHHCCC', 'CCHHHHHHHHHHHHCCC', 'CEEEEEC', 'CCCHHHHHHHHHHHHHCC', 'CCCECCEECC', 'CCCHHHCCCC', 'CCCCCCCHHHCC', 'CCCCCCCCCCCCCCCCCC', 'CECCCCCCCC', 'CCCCCCCEEC', 'CCCCCCCCECCCCCCCCCC', 'CCCCCCCCCHHHC', 'CCHHHHHHHHHCCC', 'CCHHHHHHHHCCCC', 'CCCCCCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCCCCCC', 'CEC', 'CCCCCCCCC', 'CCCCHHHHCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CECCC', 'CCCCHHHCHHHHHHHCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCEEEEEECCCCCCCECC', 'CCCCHHHHHHCCCCCCC', 'CCCCHHHHHHC', 'CCCCECCC', 'CCCCCCECCCCC', 'CCCCECCCCCECCCCC', 'CCCEEECCCCC', 'CCHHHHHHHHHHCC', 'CCCCCCCCCC', 'CCECECC', 'CECCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCEEC', 'CCCCCCCCCECCCCC', 'CHHHHHHHHHHHHHHHHHHC', 'CCCCCCCEEC', 'CHHHHHHHCCCCCCCCCCCC', 'CCCCCCECCC', 'CCEECCCCCCCCCC', 'CCCCEEEC', 'CCCCCCC', 'CCCCCHHHCHHHC', 'CCCCCCCCCCC', 'CCCCCCC', 'CCCCHHHHHHCCC', 'CCCCCEEEEC', 'CCCHHHHCHHHHHHHHCCC', 'CCHHHHHHHCCC', 'CCCECCCCCCECCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCECCCCCCC', 'CCCCCC', 'CCCCCCCCCCCC', 'CCEEEEECC', 'CCCCHHHCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CCHHHHHHCC', 'CCHHHHHHHHHCC', 'CCCCCCCCCCCCCC', 'CCEEEEEEEEEEEEC', 'CCCCCCCCCCCCCCCC', 'CCHHHCCCCCCCCCCCCCCC', 'CEECCEEECCCCCC', 'CCCCCCCCCCC', 'CCHHHHHCECC', 'CCCCCCCCCCCCCCC', 'CHHHHHHHC', 'CCCCCCCCCEEEEEEECCCC', 'CCCCCCCCCCEC', 'CCCHHHHCCHHHHCCC', 'CCCCCHHHHHHCCCC', 'CHHHHHCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCCCCCC', 'CCEECC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCECCCCC', 'CEEECCEEEC', 'CCCCCCC', 'CCCCCCCEEC', 'CCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCHHHHHHHHC', 'CEEEEEEEEEEEEEEEC', 'CHHHHHHHHHCC', 'CCCCCCCCCCCC', 'CEC', 'CCCCEECCEEEEEEC', 'CCCECCCCCCCCCCC', 'CCCCCHHHHHHHHCC', 'CEECCCCC', 'CCCCECC', 'CCEEECCCCCEEECCCCC', 'CEEECCCC', 'CCECC', 'CECC', 'CCCECC', 'CCCCEEC', 'CCCCCCCCCCCCCCCCEEC', 'CCCCCCCCC', 'CEC', 'CCHHHHCCCCCCCCCCCCC', 'CCCCCCCCCCCEC', 'CCHHHHHHHHCCCCCCC', 'CCECC', 'CCCHHHHHCCCC', 'CCCCCCCCC', 'CHHHHHHHHC', 'CCCCCCCCHHHC', 'CCCCCCCCCCCCCCCC', 'CCCCEEEEEEEEECCCC', 'CECC', 'CCEEECCHHHHCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCHHHCCCCC', 'CCCCCCCCCCCCCC', 'CCCCHHHHHHHCCCCCCCC', 'CECCHHHHCCCCCCCC', 'CCCECCCC', 'CCCCCCECCCCC', 'CCCCCCCHHHHHHHHCCCCC', 'CCHHHHHHHCC', 'CCCCCCCCCC', 'CCCCEEECCCCCCCCC', 'CCCHHHHHHHCCCC', 'CCCCECCC', 'CCCCCC', 'CCECCEEC', 'CCCCCECCHHHCCEECCCC', 'CCHHHHHHCCCCC', 'CEECCEECC', 'CCCCECCCECCCCCC', 'CCCCCEEECCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEC', 'CCCCCCCCCCC', 'CHHHHCCEECC', 'CCCCCCHHHHHHHCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCHHHHHHHHHCCCC', 'CCCHHHHHHHHCC', 'CCCCCCHHHHC', 'CCEEC', 'CCCCCC', 'CCECCCCCCCCCCCCCCCCC', 'CCCCCEEECCCCCCCC', 'CCCCHHHHHCC', 'CCCCCCCCCCCC', 'CCCCCEEECCCCCCC', 'CCCCCHHHHHHHHHHHHHCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCC', 'CECCEEEECCEEEECCEC', 'CEEC', 'CCCCCCECCCC', 'CCCCCCCCCC', 'CEEEEEC', 'CCECCCHHHHHHHHHHCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCCCCCECCCCCCCC', 'CCCCCCC', 'CCCCCCCCCCC', 'CCCCECCCC', 'CCCHHHCHHHHCCC', 'CCCCCCEEECCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCECCCC', 'CCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCHHHHHHHHC', 'CCECCCCCCCECC', 'CCCCCCEEEEC', 'CCCCCCCCCCC', 'CCHHHHHHHHHHHHHC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCEEEC', 'CCCCCCCCC', 'CCCCECC', 'CCCHHHHHHHCCC', 'CCCHHHHHHHHHECC', 'CCEEEC', 'CCCCCCCECCCECCCC', 'CCCEECCCCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCCCHHHCC', 'CCEECCEECC', 'CCCCCCCCHHHCC', 'CCCCCCCC', 'CCCCCCCC', 'CEEEEEEEEECCCC', 'CCCCCCCCHHHCCCCCCCC', 'CCCCCCCCCCCCCCCCCEEC', 'CCCCCCCCCCCCCC', 'CCCECCCCCCCECC', 'CCCCCCCCCC', 'CCCCCCCHHHHCC', 'CCEECC', 'CHHHHHHHHHHHHCCCC', 'CHHHHHHHHHHC', 'CHHHHHHHHHHHHC', 'CCCCECCCCCC', 'CHHHHHHHHHHHHHHCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CCCCECC', 'CCCCHHHHHHCCC', 'CCEECCCC', 'CCEEEC', 'CHHHHHHCCCCCCCC', 'CCCCCCCCCC', 'CCCHHHHCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CHHHCCCCCCCCCCCCC', 'CEECCC', 'CECCCCCCCC', 'CCCCECCCCCC', 'CCCCCCCEC', 'CHHHHHHCCCCC', 'CCCEECCHHHCCEECC', 'CCCCEEECCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHCC', 'CCEEEC', 'CCCCCCCCC', 'CCHHHHHHHHCCHHHC', 'CCCCCCCCCC', 'CCCEECCC', 'CHHHHHHHHHHHHCCCCCC', 'CCCCCCC', 'CCCCCCCCCCCCCCCCCCC', 'CCECCECC', 'CCCECCCC', 'CHHHHHHHHHHHHHHC', 'CCCCCCCCCC', 'CCCCCC', 'CCHHHHHHHHHHHHC', 'CCCCCCCCC', 'CCCHHHHHHHHHCCC', 'CEEEC', 'CCCCCCCEECCCC', 'CCEECCEEC', 'CCCCCCCCCC', 'CHHHHHHHHHHHHHHHC', 'CCCHHHCCCCC', 'CCCCCCCCC', 'CCCCECCCCC', 'CCCCCEEECCCCC', 'CCCCCEECCCCCCCCCCCCC', 'CCCCCCCCHHHHHHHHHC', 'CCCEEEECCEC', 'CCCCCCCCCEEC', 'CCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCEECCECCCC', 'CCCCCCCCCCC', 'CCHHHHHHHHHHHHHHHHCC', 'CCCCCCCCC', 'CCCCHHHC', 'CCCCCCCCCCHHHHCC', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCCCHHHCCCC', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCHHHHHHHCC', 'CCEEEEC', 'CCCCCCCEEEECEC', 'CCCCCCCCCCCCCC', 'CECCCC', 'CCCCCECEECCCC', 'CCEEECC', 'CCCCCCCCCCCCCCC', 'CCCCCCEEEC', 'CCCCCCCCCCC', 'CCEEEC', 'CCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CEECCECCCC', 'CCHHHCCC', 'CCCCCCCCEC', 'CCCCHHHHHHHHHHCCC', 'CCCCCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCECC', 'CCCCCCCCCCC', 'CCCCCC', 'CCECC', 'CCCCCCCCCCCCCCCCCCC', 'CECCCC', 'CCCEEEEEEEEEECCCCCC', 'CCCCCCCCCEEEEEEECCCC', 'CCCECCCC', 'CCCCCCCCCCCCC', 'CCCCCCECCC', 'CCCCCCECC', 'CCCECCCCCCCECCC', 'CCCCECCC', 'CCCCCCCCHHHHHHHEC', 'CCHHHHHHCCCC', 'CCCCCCCCCCECCCCC', 'CCCEEECCEEECC', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCHHHHHHHHHHHHHHHCCC', 'CCEEEEC', 'CCCCCCCCC', 'CCCHHHHCECCCCCCCEEC', 'CCCCCCCCC', 'CCHHHHHHHHC', 'CCCHHHHHHHHHHCCCC', 'CEEEEEEEEEEEEECC', 'CEEEC', 'CCHHHHHHHCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCEEEC', 'CCCCCCCCC', 'CCEEECCCCC', 'CCCCCCECCCC', 'CHHHHHHHHCC', 'CCCCCCCCCCCCCCCCC', 'CCCCCHHHHCC', 'CHHHHHHHHHHHHHHHHC', 'CCCEEEEECCC', 'CCCEEEEECCCCCCC', 'CEEEEEEECCEEEEEEEC', 'CCEEC', 'CCCCCCCC', 'CCCHHHHHHHHHHHCCCCC', 'CCECCECCCC', 'CCCEECC', 'CCCCCC', 'CCEEEEEECC', 'CEEECCCCEEEC', 'CCCCCC', 'CCCCCECCCCCCCCCCCCCC', 'CCCCCCC', 'CCCCCCCCCCCCHHHHCC', 'CCCCCCEEEEEEECCCCCCC', 'CCCCECCCCCC', 'CCCHHHHHHHHCC', 'CCCCCCCCC', 'CHHHHHHHHHHHHHCCCC', 'CCCEHHHCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCHHHHCCC', 'CCHHHHHHHHHHHHHHCC', 'CCCHHHHHHHHCC', 'CHHHHHHHHCHHHHCCCCCC', 'CCCECCC', 'CCCCCCEEEC', 'CCCECCEECC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCECCCC', 'CCCCCECCC', 'CCCCCCCCCCCCCCHHHCC', 'CEEEEEEEEEC', 'CCCCCCCCCCC', 'CCCCCHHHHHHHHHHCC', 'CHHHHHHHHHHHHCC', 'CCCCCCCCCC', 'CHHHHCHHHHHHHCC', 'CCEECCC', 'CCCCCHHHHHHCCCCC', 'CECCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCEEEECCCCEEEECC', 'CCCCCCCCHHHHCCCCCCC', 'CCCCCCCCCHHHHHHHCC', 'CCCECCC', 'CCCCCCCC', 'CCCCCCCCCC', 'CCCCECECCCCC', 'CECCEEEEEEECCCCCCC', 'CCCCCEEECCCCCCC', 'CCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCHHHHHHECEECCCCC', 'CCCCCCCCCCCCCC', 'CEEEEEC', 'CCEECCCC', 'CCCCHHHHHC', 'CCCCCCCCCC', 'CEEEEEEEEEEEEECC', 'CCCCCCCCCCECCCCC', 'CCCCCCCCECC', 'CEEEEC', 'CCCCEEEEECCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCHHHHHHCCCC', 'CCCCHHHCC', 'CCHHHHHHCCCCCC', 'CCEECC', 'CCCEC', 'CEEC', 'CECCCCCCCCCCC', 'CCCCHHHHCCC', 'CCCHHHHHCCCCC', 'CCCECCCCCECC', 'CHHHHHHHHHHHHHCCCCCC', 'CEEEECCCCC', 'CCCCCCCCEECC', 'CCCCCCCCCCCC', 'CCCCECC', 'CEECCCCCCCCCCCCCCCCC', 'CCECC', 'CCCCCCCCCCCCCCHHHCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCECCCCCCCC', 'CECCCCCCC', 'CCCCCCCCCHHHC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCECCCCC', 'CEEEEEEEEEC', 'CCCCCCCCCCCECC', 'CCHHHHHHHHCCHHHC', 'CCCCCCCCC', 'CCCCCHHHHHHCCCC', 'CCCCCCCCCC', 'CCCECCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCHHHHHCCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CHHHHHHHHHHHHHHC', 'CCEECCC', 'CCECC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCEECCEECCC', 'CCCCCCCCC', 'CCCCHHHHHHHHHHHHHHCC', 'CCCCHHHHHHHHHHHCCCCC', 'CHHHHHHHHCCCCCC', 'CCCCCCCC', 'CCHHHHHHHHHHHHC', 'CCECC', 'CCCCCCCCECCCCCCCCC', 'CCCCCCCCEEEEEECC', 'CCCCEEEECCCCCCC', 'CCCCCEECCCC', 'CHHHHHHHHHHHHHHHCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCECCEC', 'CCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCCHHHHHHCCCCC', 'CCCCCCCCCCCCEEECCC', 'CCCCHHHCCCCCCC', 'CCCCCECCC', 'CCCCCCCCCHHHHHHHHHHC', 'CCEEEECCCCEEEECCC', 'CCEEECC', 'CHHHHHHHHHHHHHHC', 'CCCCCCCCCCCCCCCCCCCC', 'CEEEEEECCCEEEEEC', 'CEEEEEEEEEEEEECC', 'CCCCCCHHHCC', 'CCCCCCHHHHHHHHHHCCC', 'CCEEC', 'CCCCEEEECCCCCCCC', 'CCCCCHHHHHHHHCC', 'CCCCCCCCCCCCCCCCC', 'CCHHHHHHHHHHHCCCCCC', 'CCCCCCHHHCC', 'CEECCCCEEC', 'CCCCCCCCC', 'CCCCCCCCECCCCCCCCC', 'CCCCECCCCCC', 'CCCCCCCCC', 'CEECCCCECCC', 'CCCCCEECCCCC', 'CCCCCC', 'CCCCCHHHHCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEECCCEEEEEC', 'CCCHHHHHCCCCC', 'CCCHHHHHCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCC', 'CCCECCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCCCCCCCCCCCC', 'CCECEECC', 'CCCCCCCCHHHHHCCCCCC', 'CHHHCHHHHCCC', 'CCCCCCCHHHHHHCCCCCC', 'CCCCCCC', 'CCCCEECCCCCEECCCCC', 'CCCECEEC', 'CCCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCECCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCC', 'CCCCHHHHHCCCCCCCCCC', 'CCCCCCEEEC', 'CCCECCCCEC', 'CHHHHHHHC', 'CCCCCCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCCECCCCC', 'CCCCHHHHHCCCCCC', 'CCEEEC', 'CEC', 'CCCCCHHHHHHCCCCCC', 'CCCCCCCCCCCCCCCCCC', 'CHHHHCCCCCC', 'CCCCHHHHHHHHC', 'CCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCECCCCCC', 'CCCCCCCCCCC', 'CCCCCCCCECC', 'CCCCCCCCCCCHHHC', 'CCCCCC', 'CCCCCCCCCCCCCCECC', 'CCCEEEECCHHHCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCCC', 'CECCC', 'CHHHHHHHHHHHHHCCCCC', 'CCCCCCCCCCCCCC', 'CECCCCHHHEC', 'CCCCCEECC', 'CCHHHHHHHHCCC', 'CCCCCEEEECCCC', 'CCCCCCC', 'CCCCCCCCCCCCCCC', 'CCECEEC', 'CCCCCCCCCCCCCCCCCC', 'CECECCCCCCC', 'CCCCCCCCCCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCEEECCCCCCCC', 'CCCCCCCCCC', 'CCCCECCEECC', 'CCCCCECCCCCC', 'CCCECCCCC', 'CCEC', 'CCECCCCCCC', 'CCCCCCCCCCECCC', 'CEC', 'CCCEECC', 'CEEC', 'CCCCCHHHHHHHCCCCCC', 'CCEEEC', 'CCCCEC', 'CCCCCCEECCCCCEECCCCC', 'CCCEECCCC', 'CEECCCCCCCCCC', 'CCCCCEC', 'CCCHHHHHHHHHHHHHHHCC', 'CCCCCCCC', 'CEEEEEC', 'CCEECCCCCCC', 'CCCCCCCCCC', 'CCCCCCCHHHHHHHHHHCCC', 'CCCCCCHHHCC', 'CCCEECCECCEC', 'CCEEECCCCCCCC', 'CCCCCCCCHHHCC', 'CCCCCCCCC', 'CCCCCCCCCC', 'CCCHHHHHCCCCCC', 'CCCCCHHHCCECC', 'CEC', 'CCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCECC', 'CCCCCCCCCCCCCCCCCCCC', 'CEEEEC', 'CCCCCCEEEEEEECCCC', 'CCCCCECCCCC', 'CCCCCCCCCCCCCCC', 'CCCCCC', 'CCCCCCCCCCCCCCCCCC', 'CCECCCCCCCCC', 'CCCCCCCCEECCECCCCCCC', 'CHHHHHHHHHHHHHHHHCC', 'CCCCCCCECCCC', 'CCCCCCCCCEEECCCCC', 'CCCCCCCECCCCCCC', 'CCCECCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCC', 'CCCCCHHHECCCCC', 'CCCECCEECCC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCEEC', 'CCEEECCCCCEEECC', 'CCCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCCECCCCCCHHHC', 'CCCEEEECCCCCCCC', 'CHHHHHHHHHHCHHHHC', 'CCCHHHHHHHHHCCCC', 'CCCCECCCCCC', 'CCCCCCCEECCCCCC', 'CCEECC', 'CCCCCCCHHHHCCCCCCC', 'CEEECCC', 'CCHHHHHHHHHHC', 'CCCCCCCCC', 'CHHHHHHHC', 'CCCHHHHCHHHHHHHC', 'CCCCCCC', 'CCCCHHHHCC', 'CCCEECC', 'CCEEC', 'CCCCCCCCCHHHHHHCCCC', 'CCCCCCCCCCC', 'CCECCECC', 'CCCCHHHHCCCCCC', 'CCCCCCCCC', 'CCCECCEECC', 'CCCCCCCCCCC', 'CCCEEEEEECCEEEEEECC', 'CCCCCCCHHHHHHHHHHHC', 'CECCECCC', 'CCCCCCCCC', 'CHHHHHHHHCCC', 'CCCCCCCCCCCCCC', 'CCHHHHHCCCCCC', 'CCCCCCCCCC', 'CCHHHHHHHHCCCCC', 'CCEECC', 'CCCCCCCCCC', 'CCCCCCCCCC', 'CCEHHHHCCC', 'CCCCCCCCECCCCCC', 'CEEECCCCEEEC', 'CCCHHHHHHC', 'CCCCCCCCCCCC', 'CECCCCCCCCCCCCCCCCCC', 'CCCCEEECCCCCCCC', 'CCCCCHHHHHHCCC', 'CCHHHHHHHHHHHHHHHCCC', 'CCEECCCCCC', 'CCHHHCHHHHHHHHHHHHC', 'CECCC', 'CCCCHHHCC', 'CECCCCCCCC', 'CCCCCCCCC', 'CCCCHHHHHHHHHHHHCCCC', 'CCEEEECCEEEEC', 'CHHHHHHHHHHHHHHCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCHHHHCC', 'CCCCCCCCCCCCC', 'CCCHHHHHHHHHECC', 'CCCCCCC', 'CCHHHHHHHHCCCCC', 'CCECCCCHHHHCCCCC', 'CCCCCCCCCCC', 'CCCCCCEECC', 'CCCHHHHHHECEECCCCC', 'CCCCCCCCCCCCC', 'CCEECCCCCCCCC', 'CCCCCCCCCC', 'CEEEEEEEEEEEEEC', 'CCHHHHHHHHHCC', 'CCCCCCCCCCCCC', 'CEEC', 'CCCCCCCCC', 'CCCCCCEEEEEEECCCC', 'CCCCCCCCCCCCCCCCC', 'CCCCCEECCCCHHHHCCCCC', 'CECCCEECCCCCCEEHHHC', 'CCCCCCCCC', 'CCCCCCCCCEEEEEEECCCC', 'CEEEEC', 'CCCCCCCCCECEECCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCCCC', 'CCECCC', 'CCCCCECCCCCCCCCECCC', 'CCCCCCCCCHHHHHHHCCC', 'CCCCECCCCC', 'CCCCCCCCCC', 'CCCCCCC', 'CEEEEEC', 'CCCCCCCCCCCCCC', 'CCCCCCC', 'CCCCCCCCCCCCCC', 'CCCCCCC', 'CCCCCCCCCCC', 'CCHHHHHHHHHHCCC', 'CCCHHHHHHHHCCC', 'CHHHCC', 'CCCECEECCCCCCC', 'CCCCHHHHHCCCCCC', 'CCECC', 'CCCCCEECCCCHHHHHCCCC', 'CCCCHHHHHHHCCCCC', 'CCCCCCCCCHHHHHHHHHCC', 'CCCCC', 'CCCCCCCCCCCC', 'CCCCCCECCCC', 'CCECCCCCC', 'CHHHHHHHHHCCCC', 'CCCCCCCCCCCCCCCCC', 'CCEC', 'CEEEC', 'CCCCCCCCCCC', 'CCCCCCC', 'CECCCCC', 'CCCCCCC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCEECCC', 'CCCCCCCHHHHHHHHHCCC', 'CEEEC', 'CCCCCCCCCC', 'CCCCCCCCCCCC', 'CEEEEEEEC', 'CCEEEEECCCCCCC', 'CCCCCCEEC', 'CCCCCCCCCC', 'CCECEEC', 'CECCCCCCCC', 'CECCCCC', 'CCHHHHHHHHCCCCCC', 'CCCHHHHHHHCC', 'CCCCCECCCCCCCCCCCC', 'CCHHHHHHHCC', 'CCCCCEEECCCCCCCCCCC', 'CCCCCCCCC', 'CCCCCECCCCCHHHHHCCCC', 'CCCCCCCECCCCCCEECCC', 'CCCHHHHHHHHHHHHHCCC', 'CCCCECCCCC', 'CCCCCCCCCC', 'CEEEEC', 'CCCCEEEEECCEEEEEEC', 'CCCCECCCCCC', 'CCCCCCCCCCCCCCCCCCC', 'CHHHHHHHHHHHHHHHHHC', 'CECCECCC', 'CCCCCCC', 'CCCCHHHCCCC', 'CCCCCCCCCCCCC', 'CCCEHHHHC', 'CCCCCCCCCCCCCECCC', 'CCCCCCCCCCCC', 'CCCCCCCCCEEEC', 'CCHHHHCCCC', 'CCEECC', 'CCCEEEC', 'CCCECCCCECC', 'CCCCCCCCC', 'CCCCCCCCCCC', 'CCHHHHHCCCCC', 'CCCCCC', 'CCCCCCCCCCCCCCCC', 'CCEEEECCCEEEEEEC', 'CCCCCCCCCCCCCCCCECCC', 'CCCCCCCCCCCCCCCC', 'CHHHCHHHHHCCCCCCC', 'CCCCCCECCCCCC', 'CCCEEEEC', 'CECCCCC', 'CEC', 'CCCCCCCCCCC', 'CCCCCCEECCCC', 'CCCHHHHHCC', 'CCCEECCCCCCCCCEECCC', 'CCCCECCCCC', 'CHHHHHHHHHHHCC', 'CCCC', 'CECCCCCCCCCEEECCCCCC', 'CCCCHHHHHHHHHHHHHHCC', 'CCCCCCCCC', 'CCCCCCCCC', 'CCCCCCCCCCCCECCCCCCC', 'CCHHHHHHHCCC', 'CCCCCCCCCC', 'CCCCCCCCC', 'CCCCCCC', 'CCCCCCHHHHHHHHC', 'CCCHHHHHHHCCCC', 'CCCCHHHHCCCCC', 'CCCECCC', 'CCCCCCCCCCCCCCCCCC', 'CCCCCCCCCCC', 'CCEHHHCC', 'CCCECCCCCC', 'CHHHHHHHHHHCC', 'CEEEEEEECCC', 'CCCCCCCCCCCCCCCCCC', 'CCCECCEEEC', 'CEECHHHCEEEEEEEECEC', 'CHHHHHHHHHHHHHHHC', 'CCCCCCCCC', 'CCCHHHHHCCCC', 'CCCCCCCCC', 'CHHHHCCCCCCCCCCC', 'CCHHHCCECCCCCCEECCC', 'CEEEECCCCC', 'CHHHHHHHHHHCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCEEECCCECCCCCCC', 'CCCCCCHHHCC', 'CCCCEEECCCC', 'CEEEEC', 'CEEEECCCCC', 'CCCCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCCECCCCC', 'CCCCCEEHHHHHHHHCC', 'CCEECCCCCC', 'CCCCCCCCCCCCCCCCCCCC', 'CCCCCHHHCCC', 'CCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCHHHHCC', 'CCCCCCCCCCCCCEEEC', 'CCEECCCCCCCCCCCC', 'CCCCCCCCCCCCCCCCC', 'CEECCCCC', 'CCCCHHHHHHCC', 'CCCCCCCCCCCCCCCCCCC', 'CCEEEEEECC', 'CCCHHHHHCCCCC', 'CCEEEEEEEEEEEECC', 'CCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CCEECCCC', 'CCCCCCCCCCC', 'CCCCEEECC', 'CCCCCCECC', 'CCCCCCCCHHHHHHHHHC', 'CCCCCCHHHHHHHHHHCC', 'CCCCCCCCCCCCCCECC', 'CCCCCCCCCHHHHHHHHCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCC', 'CCCEECC', 'CHHHHHHHHHHHHHCCCCC', 'CECCCECCCCCCCCCCCCCC', 'CCCCCHHHHCCCCCC', 'CCCHHHCECCCCCCCC', 'CCCCCCCEECC', 'CCCCCC', 'CCCCCCCCCCCCC', 'CCCHHHHHCCCCC', 'CCCHHHHHHHHHHHHCCCCC', 'CCEEC', 'CCEEEEECCCEEEEECC', 'CCCCCHHHHCCCCCCCC', 'CEEECCEECC', 'CCCCCCEEEECCCCCCC', 'CCCCCCCCC', 'CECCCCCCCCCC', 'CCECCC', 'CCCCCCCCCCCC', 'CHHHHHHHHHHHHHCCCCCC', 'CCCCCCCCCC', 'CCCCCCCCCCCCCCCC', 'CCCCCCCCCCCCC', 'CEECCCCCCCCCCCCCCCCC', 'CCCCCCCCCCCC', 'CCCCCHHHHHCCC', 'CHHHHHHHHHHHHCCCCCCC', 'CCECCCCCCCCCCCCCCCCC', 'CCCCCCCEEC', 'CHHHHHHHHHHHHC', 'CCHHHHHHHHHCCCC', 'CCCCCCCCC', 'CCCEEEEC', 'CCCCCEEECC', 'CCECC', 'CCCCCCCHHHHCCCCCHHHC']\n"
     ]
    }
   ],
   "source": [
    "target=[]\n",
    "for idx in y_val:\n",
    "    decoded_structure = []\n",
    "    for id in idx:\n",
    "        if id.item() == EOS_token:\n",
    "            break\n",
    "        decoded_structure.append(output_lang.index2word[id.item()])\n",
    "    target.append(\"\".join(decoded_structure))\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end I calculated the accuracy at the character and sentence level. However I think the character-level statistic is by far more important as single mistakes are inevitable with such long and repetitive sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level accuracy: 42.13788322906667%\n",
      "Exact match: 0.0%\n"
     ]
    }
   ],
   "source": [
    "def char_level_acc(predictions, targets):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for pred, target in zip(list(predictions), list(targets)):\n",
    "        if len(pred)<len(target):\n",
    "            pred = pred + (\"$\" * (len(target)-len(pred)))\n",
    "        if len(pred)>len(target):\n",
    "            target = target + (\"$\" * (len(pred)-len(target)))\n",
    "\n",
    "        accuracy += accuracy_score(list(pred),list(target))\n",
    "\n",
    "    return accuracy / len(predictions)\n",
    "\n",
    "print(f'Character-level accuracy: {char_level_acc(pred, target)*100}%')\n",
    "print(f'Exact match: {accuracy_score(pred,target)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
