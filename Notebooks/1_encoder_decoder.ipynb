{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = pd.read_csv(\"../Data/2018-06-06-ss.cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>chain_code</th>\n",
       "      <th>seq</th>\n",
       "      <th>sst8</th>\n",
       "      <th>sst3</th>\n",
       "      <th>len</th>\n",
       "      <th>has_nonstd_aa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1A30</td>\n",
       "      <td>C</td>\n",
       "      <td>EDL</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1B05</td>\n",
       "      <td>B</td>\n",
       "      <td>KCK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1B0H</td>\n",
       "      <td>B</td>\n",
       "      <td>KAK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1B1H</td>\n",
       "      <td>B</td>\n",
       "      <td>KFK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1B2H</td>\n",
       "      <td>B</td>\n",
       "      <td>KAK</td>\n",
       "      <td>CBC</td>\n",
       "      <td>CEC</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393727</th>\n",
       "      <td>4UWE</td>\n",
       "      <td>D</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCBTTCEEEEEEEEEETTEEEEEEEECCCSSCCB...</td>\n",
       "      <td>CCCCCCCCCCCCCCECCCEEEEEEEEEECCEEEEEEEECCCCCCCE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393728</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>A</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393729</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>B</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393730</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>C</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393731</th>\n",
       "      <td>5J8V</td>\n",
       "      <td>D</td>\n",
       "      <td>MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...</td>\n",
       "      <td>CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...</td>\n",
       "      <td>5037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393732 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pdb_id chain_code                                                seq  \\\n",
       "0        1A30          C                                                EDL   \n",
       "1        1B05          B                                                KCK   \n",
       "2        1B0H          B                                                KAK   \n",
       "3        1B1H          B                                                KFK   \n",
       "4        1B2H          B                                                KAK   \n",
       "...       ...        ...                                                ...   \n",
       "393727   4UWE          D  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393728   5J8V          A  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393729   5J8V          B  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393730   5J8V          C  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "393731   5J8V          D  MGDGGEGEDEVQFLRTDDEVVLQCSATVLKEQLKLCLAAEGFGNRL...   \n",
       "\n",
       "                                                     sst8  \\\n",
       "0                                                     CBC   \n",
       "1                                                     CBC   \n",
       "2                                                     CBC   \n",
       "3                                                     CBC   \n",
       "4                                                     CBC   \n",
       "...                                                   ...   \n",
       "393727  CCCCCCCCCCCCCCBTTCEEEEEEEEEETTEEEEEEEECCCSSCCB...   \n",
       "393728  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393729  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393730  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "393731  CCCCCCCCCCCCCCCSSSCCEEEECSEETTEECCEECCEEETTEEE...   \n",
       "\n",
       "                                                     sst3   len  has_nonstd_aa  \n",
       "0                                                     CEC     3          False  \n",
       "1                                                     CEC     3          False  \n",
       "2                                                     CEC     3          False  \n",
       "3                                                     CEC     3          False  \n",
       "4                                                     CEC     3          False  \n",
       "...                                                   ...   ...            ...  \n",
       "393727  CCCCCCCCCCCCCCECCCEEEEEEEEEECCEEEEEEEECCCCCCCE...  5037           True  \n",
       "393728  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393729  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393730  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "393731  CCCCCCCCCCCCCCCCCCCCEEEECCEECCEECCEECCEEECCEEE...  5037          False  \n",
       "\n",
       "[393732 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with sampling the dataset: \n",
    "- selected sequences no longer than 20 amino acids (in this architecture I used shorter sequences than in LSTM because of the resources needed to train on longer sequences)\n",
    "- selected the important columns\n",
    "- deduplicated dataset\n",
    "- removed sequences that were only composed of \"*\" sign - which indicated nonstandard amino acids (B, O, U, X, or Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = proteins[\n",
    "    (proteins[\"len\"]>=1) &\n",
    "    (proteins[\"len\"]<=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[[\"seq\",\"sst3\",\"sst8\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"len\"] = sample[\"seq\"].apply(len)\n",
    "sample = sample.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the seq2seq tutorial (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) I created Lang class that will store the input \"language\" - in my case amino acid sequence and the output \"language\" - three-state (Q3) secondary structure\n",
    "\n",
    "The sequences and structures are characterised by their own set of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in list(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "Sequence: 23\n",
      "Structure: 5\n",
      "('*YSPTSPSYSPTSPS', 'CCCCCCCECCCCCCC')\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang() \n",
    "\n",
    "    pairs = list(zip(lang1,lang2))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(f\"Sequence: {input_lang.n_words}\")\n",
    "    print(f\"Structure: {output_lang.n_words}\")\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(sample[\"seq\"], sample[\"sst3\"])\n",
    "\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 53524, 'E': 7831, 'H': 12860}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'E': 3, 'H': 4}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = sample[\"len\"].max()+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in list(sentence)]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "n = len(pairs)\n",
    "input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_lang, inp)\n",
    "    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "    inp_ids.append(EOS_token)\n",
    "    tgt_ids.append(EOS_token)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(input_ids)*0.6)\n",
    "test_size = int(len(input_ids)*0.2)\n",
    "\n",
    "X = input_ids\n",
    "y = target_ids\n",
    "\n",
    "X_train = torch.tensor(X[:train_size], dtype=torch.long)\n",
    "y_train = torch.tensor(y[:train_size],dtype=torch.long)\n",
    "\n",
    "X_test = torch.tensor(X[train_size:train_size+test_size],dtype=torch.long)\n",
    "y_test = torch.tensor(y[train_size:train_size+test_size],dtype=torch.long)\n",
    "\n",
    "X_val = torch.tensor(X[train_size+test_size:],dtype=torch.long)\n",
    "y_val = torch.tensor(y[train_size+test_size:],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to calculate weights of the output characters, and use them later in the loss function. The frequencies of the characters in the structure vary a lot with C letter being the most frequent. I wanted to avoid the situation where model learns only to output the majority group, so I calculated the reverse probability of the word frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_freq = (torch.tensor(y, dtype=torch.long).shape[0] * torch.tensor(y, dtype=torch.long).shape[1]) - torch.count_nonzero(torch.tensor(y, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0468, 0.4058, 0.0456, 0.3119, 0.1899])\n"
     ]
    }
   ],
   "source": [
    "vocab = output_lang.word2index\n",
    "word_freq = output_lang.word2count\n",
    "\n",
    "vocab.update({\"SOS\":0,\"EOS\":1})\n",
    "word_freq.update({\"SOS\":int(SOS_freq),\"EOS\":len(sample)})\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "weights = torch.zeros(vocab_size)\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    weights[idx] = 1.0 / (word_freq[word]) \n",
    "    \n",
    "weights = weights / weights.sum()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I based the model on seq2seq tutorial (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
    "\n",
    "I tested the result with the presented architecture as well as using the decoder without attention mechanism. The results were better with attention. Then I used bidirectional encoder which led to better results. I also decided to add embedding size parameter in the decoder to be able to adjust its size independently from hidden size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True, num_layers=1)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "\n",
    "        hidden = torch.sum(hidden,dim=0).unsqueeze(dim=0)\n",
    "        output = torch.chunk(output, 2 , dim = 2)[0] + torch.chunk(output, 2 , dim = 2)[1]\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_dim+hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, decoder_input, hidden, encoder_outputs):\n",
    "        output = self.embedding(decoder_input)\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((output, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experimented with learning rate, decided to use large batch size to speed up learning process, because the output was not highly dimensional I decided to use small hidden size and embedding.\n",
    "\n",
    "Finally in the training loop I implemented early stopping and saving best model, I wanted to be able to optimise the number of epochs and prevent overfitting, so if during the training the train loss is decreasing but test loss is rising I stop the training, and save the model that was having the lowest test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "batch_size = 128\n",
    "hidden_size = 32\n",
    "embedding_size = 32\n",
    "n_epochs = 100\n",
    "\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(X_train,y_train)), batch_size=batch_size)\n",
    "test_loader = DataLoader(list(zip(X_test,y_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, embedding_size, output_lang.n_words)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.325981765985489, Test loss: 1.0632933378219604\n",
      "Epoch: 1, Train loss: 0.7983005387442452, Test loss: 0.704858660697937\n",
      "Epoch: 2, Train loss: 0.5554359619106565, Test loss: 0.5345368981361389\n",
      "Epoch: 3, Train loss: 0.4426747815949576, Test loss: 0.4648110568523407\n",
      "Epoch: 4, Train loss: 0.3920128984110696, Test loss: 0.41604119539260864\n",
      "Epoch: 5, Train loss: 0.3603114549602781, Test loss: 0.38830944895744324\n",
      "Epoch: 6, Train loss: 0.34307172362293514, Test loss: 0.37202373147010803\n",
      "Epoch: 7, Train loss: 0.33369557027305874, Test loss: 0.36122068762779236\n",
      "Epoch: 8, Train loss: 0.3210261049015181, Test loss: 0.3555908203125\n",
      "Epoch: 9, Train loss: 0.3136281041162355, Test loss: 0.34400221705436707\n",
      "Epoch: 10, Train loss: 0.30405750977141516, Test loss: 0.3403071165084839\n",
      "Epoch: 11, Train loss: 0.3226709280695234, Test loss: 0.3445257246494293\n",
      "Epoch: 12, Train loss: 0.3008771411010197, Test loss: 0.33709049224853516\n",
      "Epoch: 13, Train loss: 0.29378327088696615, Test loss: 0.3268061578273773\n",
      "Epoch: 14, Train loss: 0.2888613609330995, Test loss: 0.32368046045303345\n",
      "Epoch: 15, Train loss: 0.2889133489557675, Test loss: 0.3274241089820862\n",
      "Epoch: 16, Train loss: 0.2854823278529303, Test loss: 0.32316625118255615\n",
      "Epoch: 17, Train loss: 0.28315176921231405, Test loss: 0.32466110587120056\n",
      "Epoch: 18, Train loss: 0.28082708535449846, Test loss: 0.31502485275268555\n",
      "Epoch: 19, Train loss: 0.27706671985132353, Test loss: 0.31591564416885376\n",
      "Epoch: 20, Train loss: 0.2755127857838358, Test loss: 0.31116729974746704\n",
      "Epoch: 21, Train loss: 0.2733310205595834, Test loss: 0.3089085817337036\n",
      "Epoch: 22, Train loss: 0.275595348328352, Test loss: 0.31236031651496887\n",
      "Epoch: 23, Train loss: 0.4343397915363312, Test loss: 0.37976473569869995\n"
     ]
    }
   ],
   "source": [
    "best_result = np.inf\n",
    "test_loss_array = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(X_batch)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, y_batch)\n",
    "\n",
    "        loss = loss_fn(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            y_batch.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  \n",
    "            \n",
    "            for X_batch, y_batch in test_loader:\n",
    "\n",
    "                encoder_outputs, encoder_hidden = encoder(X_batch)\n",
    "                decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, y_batch)\n",
    "                t_loss = loss_fn(decoder_outputs.view(-1, decoder_outputs.size(-1)),y_batch.view(-1))\n",
    "\n",
    "                test_loss+=t_loss\n",
    "\n",
    "    loss = total_loss / (len(X_train) // batch_size)\n",
    "    loss_test = test_loss / (len(y_test) // batch_size)\n",
    "\n",
    "    test_loss_array.append(loss_test)\n",
    "\n",
    "    if loss_test < best_result:\n",
    "        torch.save(encoder.state_dict(), \"./encoder.pth\")\n",
    "        torch.save(decoder.state_dict(), \"./decoder.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train loss: {loss}, Test loss: {loss_test}\")\n",
    "\n",
    "    if len(test_loss_array)>patience+1:\n",
    "        if not (any(x > test_loss_array[-1] for x in test_loss_array[len(test_loss_array)-patience-1:-1])):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I loaded the best model and used it for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(\"../Models/encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"../Models/decoder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCCCCCCCC', 'CHHHHHH', 'CHHHHHHHHH', 'CCCCCCCCCCCCCC', 'CCCCCCCCCC']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(X_val)\n",
    "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "\n",
    "    pred = []\n",
    "    for idx in decoded_ids:\n",
    "        decoded_structure = []\n",
    "        for id in idx:\n",
    "            if id.item() == EOS_token or id.item() == SOS_token:\n",
    "                break\n",
    "            decoded_structure.append(output_lang.index2word[id.item()])\n",
    "        pred.append(\"\".join(decoded_structure))\n",
    "    \n",
    "    print(pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCCCCCCCCCCCCC', 'CCCCCCCC', 'CCCCCECCCCCC', 'CCCHHHHHHHHHHHCCCCC', 'CCCCCCCCCCCCC']\n"
     ]
    }
   ],
   "source": [
    "target=[]\n",
    "for idx in y_val:\n",
    "    decoded_structure = []\n",
    "    for id in idx:\n",
    "        if id.item() == EOS_token:\n",
    "            break\n",
    "        decoded_structure.append(output_lang.index2word[id.item()])\n",
    "    target.append(\"\".join(decoded_structure))\n",
    "\n",
    "print(target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end I calculated the accuracy at the character and sentence level. However I think the character-level statistic is by far more important as single mistakes are inevitable with such long and repetitive sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level accuracy: 42.13788322906667%\n",
      "Exact match: 0.0%\n"
     ]
    }
   ],
   "source": [
    "def char_level_acc(predictions, targets):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for pred, target in zip(list(predictions), list(targets)):\n",
    "        if len(pred)<len(target):\n",
    "            pred = pred + (\"$\" * (len(target)-len(pred)))\n",
    "        if len(pred)>len(target):\n",
    "            target = target + (\"$\" * (len(pred)-len(target)))\n",
    "\n",
    "        accuracy += accuracy_score(list(pred),list(target))\n",
    "\n",
    "    return accuracy / len(predictions)\n",
    "\n",
    "print(f'Character-level accuracy: {char_level_acc(pred, target)*100}%')\n",
    "print(f'Exact match: {accuracy_score(pred,target)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
